# Ciclo de Vida do Treinamento do Modelo LSTM

> Documento tÃ©cnico detalhando todo o processo de treinamento do modelo de previsÃ£o de preÃ§os de aÃ§Ãµes PETR4.SA

**Data:** 19 de Fevereiro de 2026  
**Projeto:** Tech Challenge - Fase 4  
**Autor:** Desenvolvido com assistÃªncia de IA

---

## Ãndice

1. [VisÃ£o Geral do Projeto](#1-visÃ£o-geral-do-projeto)
2. [ConfiguraÃ§Ã£o Inicial](#2-configuraÃ§Ã£o-inicial)
3. [Arquitetura do Modelo - VersÃ£o 1](#3-arquitetura-do-modelo---versÃ£o-1)
4. [Primeiro Treinamento](#4-primeiro-treinamento)
5. [AvaliaÃ§Ã£o Inicial](#5-avaliaÃ§Ã£o-inicial)
6. [Processo de OtimizaÃ§Ã£o](#6-processo-de-otimizaÃ§Ã£o)
7. [Modelo Otimizado - VersÃ£o Final](#7-modelo-otimizado---versÃ£o-final)
8. [Comparativo de Resultados](#8-comparativo-de-resultados)
9. [LiÃ§Ãµes Aprendidas](#9-liÃ§Ãµes-aprendidas)
10. [GlossÃ¡rio](#10-glossÃ¡rio)

---

## 1. VisÃ£o Geral do Projeto

### 1.1 O que estamos construindo?

Um **modelo de Machine Learning** capaz de prever o preÃ§o de fechamento de uma aÃ§Ã£o (PETR4.SA - Petrobras) com base nos Ãºltimos 60 dias de histÃ³rico.

### 1.2 Por que LSTM?

**LSTM (Long Short-Term Memory)** Ã© um tipo especial de rede neural recorrente (RNN) projetada para aprender dependÃªncias de longo prazo em sequÃªncias de dados.

```
Por que nÃ£o usar uma rede neural comum?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Redes neurais tradicionais (feedforward) tratam cada entrada de forma independente.
Elas nÃ£o "lembram" o que viram antes.

PreÃ§os de aÃ§Ãµes sÃ£o SEQUÃŠNCIAS TEMPORAIS:
- O preÃ§o de hoje depende do preÃ§o de ontem
- TendÃªncias se formam ao longo de dias/semanas
- PadrÃµes sazonais existem

A LSTM resolve isso mantendo uma "memÃ³ria" interna que persiste ao longo do tempo.
```

### 1.3 Objetivo de Performance

| MÃ©trica | Significado | Meta |
|---------|-------------|------|
| **MAPE** | Erro percentual mÃ©dio | < 5% (Excelente) |
| **RMSE** | Erro mÃ©dio em R$ | < R$ 1,00 |

---

## 2. ConfiguraÃ§Ã£o Inicial

### 2.1 Dados Utilizados

```
Fonte:        Yahoo Finance (via biblioteca yfinance)
Ativo:        PETR4.SA (Petrobras - AÃ§Ã£o Preferencial)
PerÃ­odo:      2018-01-01 a 2024-01-01 (6 anos)
Registros:    1.487 dias de negociaÃ§Ã£o
Feature:      PreÃ§o de Fechamento (Close)
```

### 2.2 Por que escolhemos PETR4.SA?

1. **Liquidez**: Uma das aÃ§Ãµes mais negociadas da B3
2. **Volatilidade**: VariaÃ§Ã£o suficiente para o modelo aprender padrÃµes
3. **HistÃ³rico longo**: 6 anos de dados disponÃ­veis
4. **RelevÃ¢ncia**: Empresa brasileira de grande porte

### 2.3 PrÃ©-processamento dos Dados

#### Etapa 1: NormalizaÃ§Ã£o

```python
# O que fizemos:
scaler = MinMaxScaler(feature_range=(0, 1))
data_scaled = scaler.fit_transform(data)

# Por que normalizar?
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PreÃ§os originais variam de R$ 3,24 a R$ 27,38
# Redes neurais funcionam melhor com valores entre 0 e 1
# 
# Analogia: Ã‰ como converter temperaturas de Fahrenheit para uma escala de 0-1
# O nÃºmero muda, mas a informaÃ§Ã£o permanece a mesma
```

#### Etapa 2: CriaÃ§Ã£o de Janelas Temporais

```python
# O que fizemos:
SEQ_LENGTH = 60  # 60 dias de histÃ³rico

# Exemplo visual:
# Dados: [D1, D2, D3, ..., D60, D61, D62, ...]
#
# Amostra 1: X = [D1...D60]   â†’ y = D61 (prever)
# Amostra 2: X = [D2...D61]   â†’ y = D62 (prever)
# Amostra 3: X = [D3...D62]   â†’ y = D63 (prever)

# Por que 60 dias?
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 60 dias Ãºteis â‰ˆ 3 meses de mercado
# Captura tendÃªncias de curto/mÃ©dio prazo
# PadrÃ£o comum na literatura de previsÃ£o de aÃ§Ãµes
```

#### Etapa 3: DivisÃ£o Treino/Teste

```python
# O que fizemos:
split = int(len(X) * 0.8)  # 80% treino, 20% teste

# Resultado:
# Treino: 1.141 amostras (usadas para o modelo aprender)
# Teste:  286 amostras (usadas para avaliar se aprendeu de verdade)

# Por que dividir?
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Se usÃ¡ssemos 100% para treinar, nÃ£o saberÃ­amos se o modelo:
# - Realmente aprendeu padrÃµes, OU
# - Apenas "decorou" os dados (overfitting)
#
# O conjunto de teste sÃ£o dados que o modelo NUNCA viu durante o treino
```

---

## 3. Arquitetura do Modelo - VersÃ£o 1

### 3.1 ConfiguraÃ§Ã£o Inicial Escolhida

```python
StockLSTM(
    input_size=1,      # 1 feature (apenas preÃ§o Close)
    hidden_size=50,    # 50 neurÃ´nios na camada oculta
    num_layers=2,      # 2 camadas LSTM empilhadas
    dropout=0.2        # 20% de dropout (regularizaÃ§Ã£o)
)
```

### 3.2 Por que esses valores?

| ParÃ¢metro | Valor | Justificativa |
|-----------|-------|---------------|
| `input_size=1` | 1 | Usamos apenas o preÃ§o Close como entrada |
| `hidden_size=50` | 50 | Valor padrÃ£o recomendado para comeÃ§ar - suficiente para capturar padrÃµes bÃ¡sicos |
| `num_layers=2` | 2 | Duas camadas permitem aprender padrÃµes mais abstratos sem ser muito complexo |
| `dropout=0.2` | 20% | Taxa padrÃ£o de regularizaÃ§Ã£o - previne overfitting sem ser muito agressivo |

### 3.3 Fluxo de Dados no Modelo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ARQUITETURA StockLSTM v1                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  ENTRADA                                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€                                                            â”‚
â”‚  Shape: (batch_size, 60, 1)                                         â”‚
â”‚  Significado: N amostras, cada uma com 60 dias, 1 feature           â”‚
â”‚                                                                     â”‚
â”‚         â”‚                                                           â”‚
â”‚         â–¼                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚           LSTM Camada 1                 â”‚                        â”‚
â”‚  â”‚   â€¢ 50 neurÃ´nios (hidden_size)          â”‚                        â”‚
â”‚  â”‚   â€¢ Processa os 60 dias sequencialmente â”‚                        â”‚
â”‚  â”‚   â€¢ Aprende padrÃµes de curto prazo      â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚         â”‚                                                           â”‚
â”‚         â–¼                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚           LSTM Camada 2                 â”‚                        â”‚
â”‚  â”‚   â€¢ 50 neurÃ´nios (hidden_size)          â”‚                        â”‚
â”‚  â”‚   â€¢ Refina os padrÃµes da camada 1       â”‚                        â”‚
â”‚  â”‚   â€¢ Aprende padrÃµes mais abstratos      â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚         â”‚                                                           â”‚
â”‚         â–¼                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚           Dropout (20%)                 â”‚                        â”‚
â”‚  â”‚   â€¢ Desliga 20% dos neurÃ´nios           â”‚                        â”‚
â”‚  â”‚   â€¢ Apenas durante o treino             â”‚                        â”‚
â”‚  â”‚   â€¢ Previne overfitting                 â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚         â”‚                                                           â”‚
â”‚         â–¼                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚           Linear (50 â†’ 1)               â”‚                        â”‚
â”‚  â”‚   â€¢ Converte 50 valores em 1            â”‚                        â”‚
â”‚  â”‚   â€¢ SaÃ­da: preÃ§o previsto (normalizado) â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚         â”‚                                                           â”‚
â”‚         â–¼                                                           â”‚
â”‚  SAÃDA                                                              â”‚
â”‚  â”€â”€â”€â”€â”€                                                              â”‚
â”‚  Shape: (batch_size, 1)                                             â”‚
â”‚  Significado: 1 preÃ§o previsto para cada amostra                    â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.4 Contagem de ParÃ¢metros

```
Modelo v1 (hidden_size=50):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LSTM Camada 1:  4 Ã— (1Ã—50 + 50Ã—50 + 50) = 10.400 parÃ¢metros
LSTM Camada 2:  4 Ã— (50Ã—50 + 50Ã—50 + 50) = 20.400 parÃ¢metros
Linear:         50 Ã— 1 + 1 = 51 parÃ¢metros
Dropout:        0 parÃ¢metros (nÃ£o tem pesos)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:          ~31.000 parÃ¢metros treinÃ¡veis

Por que "4 Ã—" na LSTM?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
A LSTM tem 4 "portÃµes" internos:
1. Forget Gate (o que esquecer)
2. Input Gate (o que adicionar)
3. Cell Gate (novo conteÃºdo)
4. Output Gate (o que mostrar)
Cada portÃ£o tem seus prÃ³prios pesos, por isso multiplicamos por 4.
```

---

## 4. Primeiro Treinamento

### 4.1 ConfiguraÃ§Ã£o de Treinamento

```python
# HiperparÃ¢metros escolhidos
EPOCHS = 100           # 100 passagens pelos dados
LEARNING_RATE = 0.001  # Taxa de aprendizado (padrÃ£o do Adam)
BATCH_SIZE = None      # Batch gradient descent (todos os dados de uma vez)

# FunÃ§Ã£o de perda
criterion = nn.MSELoss()  # Mean Squared Error

# Otimizador
optimizer = optim.Adam(model.parameters(), lr=0.001)
```

### 4.2 Por que essas escolhas?

#### MSELoss (Mean Squared Error)

```
FÃ³rmula: MSE = (1/n) Ã— Î£(previsÃ£o - real)Â²

Por que usar MSE?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Problema de REGRESSÃƒO (prever nÃºmero, nÃ£o classificar)
2. Penaliza erros grandes mais do que erros pequenos
3. Derivada suave (facilita o gradiente descendente)

Alternativas consideradas:
â€¢ MAE (Mean Absolute Error) - Menos sensÃ­vel a outliers
â€¢ Huber Loss - Combina MSE e MAE
â€¢ Escolhemos MSE por ser o padrÃ£o para sÃ©ries temporais
```

#### Otimizador Adam

```
Adam = Adaptive Moment Estimation

Por que Adam e nÃ£o SGD?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Adam ajusta automaticamente a taxa de aprendizado por parÃ¢metro
2. Usa "momento" para evitar ficar preso em mÃ­nimos locais
3. Funciona bem com LSTMs (recomendaÃ§Ã£o da literatura)
4. Menos sensÃ­vel Ã  escolha do learning rate inicial

Learning Rate = 0.001
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Valor padrÃ£o do Adam. Nem muito rÃ¡pido (instÃ¡vel) nem muito lento (demora).
```

### 4.3 Loop de Treinamento Explicado

```python
for epoch in range(100):
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # FASE 1: TREINO
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    model.train()  
    # O que faz: Ativa o modo de treinamento
    # Por que: Dropout sÃ³ funciona durante o treino
    
    outputs = model(X_train)  
    # O que faz: Forward pass - dados entram, previsÃµes saem
    # Internamente: Dados passam pelas LSTMs e camada linear
    
    loss = criterion(outputs, y_train)  
    # O que faz: Calcula o erro (MSE entre previsÃµes e valores reais)
    # Resultado: Um nÃºmero que representa "quÃ£o errado" o modelo estÃ¡
    
    optimizer.zero_grad()  
    # O que faz: Limpa os gradientes da iteraÃ§Ã£o anterior
    # Por que: PyTorch ACUMULA gradientes por padrÃ£o
    #          Sem isso, gradientes antigos se misturam com novos
    
    loss.backward()  
    # O que faz: Backpropagation - calcula gradientes
    # Internamente: Para cada peso, calcula âˆ‚loss/âˆ‚peso
    #               (quanto o loss muda se o peso mudar)
    
    optimizer.step()  
    # O que faz: Atualiza os pesos usando os gradientes
    # FÃ³rmula: peso_novo = peso_antigo - learning_rate Ã— gradiente
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # FASE 2: VALIDAÃ‡ÃƒO
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    model.eval()  
    # O que faz: Ativa o modo de avaliaÃ§Ã£o
    # Por que: Desativa dropout (usa todos os neurÃ´nios)
    
    with torch.no_grad():  
        # O que faz: Desativa cÃ¡lculo de gradientes
        # Por que: Economiza memÃ³ria, nÃ£o vamos fazer backward()
        
        val_outputs = model(X_test)
        val_loss = criterion(val_outputs, y_test)
    
    # Registrar para anÃ¡lise posterior
    train_losses.append(loss.item())
    val_losses.append(val_loss.item())
```

### 4.4 Resultados do Primeiro Treinamento

```
============================================================
ğŸ‹ï¸ TREINAMENTO v1 (hidden_size=50)
============================================================

ConfiguraÃ§Ã£o:
â€¢ Dispositivo: CPU
â€¢ Ã‰pocas: 100
â€¢ Learning Rate: 0.001

Progresso:
Epoch [ 10/100] | Train Loss: 0.027582 | Val Loss: 0.293079
Epoch [ 20/100] | Train Loss: 0.019323 | Val Loss: 0.150479
Epoch [ 50/100] | Train Loss: 0.012421 | Val Loss: 0.150984
Epoch [100/100] | Train Loss: 0.002085 | Val Loss: 0.003514

Tempo total: 18.4 segundos
```

---

## 5. AvaliaÃ§Ã£o Inicial

### 5.1 MÃ©tricas Calculadas (Modelo v1)

```
============================================================
ğŸ“Š MÃ‰TRICAS DE AVALIAÃ‡ÃƒO - MODELO v1
============================================================

MSE  (Mean Squared Error):     2.0468
RMSE (Root Mean Squared Error): R$ 1.43
MAE  (Mean Absolute Error):     R$ 1.15
MAPE (Mean Absolute % Error):   6.74%

DiagnÃ³stico: BOM (MAPE entre 5-10%)
```

### 5.2 O que cada mÃ©trica significa?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INTERPRETAÃ‡ÃƒO DAS MÃ‰TRICAS                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  MSE = 2.0468                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚  MÃ©dia dos erros ao quadrado.                                   â”‚
â”‚  Unidade: (R$)Â² - difÃ­cil de interpretar diretamente            â”‚
â”‚                                                                 â”‚
â”‚  RMSE = R$ 1.43                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚  Raiz do MSE. Erro mÃ©dio na mesma unidade dos dados.            â”‚
â”‚  Significa: "Em mÃ©dia, o modelo erra R$ 1.43 por previsÃ£o"      â”‚
â”‚                                                                 â”‚
â”‚  MAE = R$ 1.15                                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚  MÃ©dia dos erros absolutos (sem elevar ao quadrado).            â”‚
â”‚  Menos sensÃ­vel a erros grandes que o RMSE.                     â”‚
â”‚                                                                 â”‚
â”‚  MAPE = 6.74%                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚  Erro percentual mÃ©dio.                                         â”‚
â”‚  Significa: "Em mÃ©dia, o modelo erra 6.74% do valor real"       â”‚
â”‚  Exemplo: Se aÃ§Ã£o vale R$ 20, erro mÃ©dio Ã© R$ 1.35              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Escala de qualidade (MAPE):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
< 5%     â†’ Excelente
5-10%    â†’ Bom        â† Nosso modelo v1 (6.74%)
10-20%   â†’ AceitÃ¡vel
20-50%   â†’ RazoÃ¡vel
> 50%    â†’ Ruim
```

### 5.3 DecisÃ£o: Modelo estÃ¡ bom, mas pode melhorar

```
AnÃ¡lise do resultado:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… MAPE de 6.74% estÃ¡ na faixa "Bom"
âœ… O modelo aprendeu (loss diminuiu)
âœ… NÃ£o hÃ¡ overfitting severo (val_loss acompanha train_loss)

âš ï¸ MAS... MAPE estÃ¡ a 1.74% de ser "Excelente" (< 5%)
âš ï¸ Erro mÃ©dio de R$ 1.43 Ã© significativo para day trading

DECISÃƒO: Vamos tentar otimizar para alcanÃ§ar MAPE < 5%
```

---

## 6. Processo de OtimizaÃ§Ã£o

### 6.1 EstratÃ©gia de OtimizaÃ§Ã£o

```
O que podemos ajustar (hiperparÃ¢metros):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. hidden_size    â†’ Capacidade do modelo (mais neurÃ´nios = mais capacidade)
2. learning_rate  â†’ Velocidade de aprendizado
3. epochs         â†’ Quantas vezes ver os dados
4. dropout        â†’ RegularizaÃ§Ã£o (evitar overfitting)
5. num_layers     â†’ Profundidade do modelo

Ordem de prioridade (maior impacto primeiro):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Learning Rate
2. Hidden Size
3. Epochs
4. Dropout
```

### 6.2 Experimentos Realizados

Executamos 12 experimentos sistemÃ¡ticos variando um parÃ¢metro por vez e depois combinaÃ§Ãµes:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  RESULTADOS DOS EXPERIMENTOS                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Experimento      â”‚  MAPE   â”‚  RMSE   â”‚  Resultado              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  Baseline         â”‚  5.45%  â”‚ R$1.36  â”‚  âš ï¸ Quase               â”‚
â”‚  LR 0.0005        â”‚  4.34%  â”‚ R$1.00  â”‚  âœ… < 5%!               â”‚
â”‚  LR 0.0001        â”‚ 51.93%  â”‚ R$11.15 â”‚  âŒ Muito lento         â”‚
â”‚  150 epochs       â”‚  4.41%  â”‚ R$1.06  â”‚  âœ… < 5%!               â”‚
â”‚  200 epochs       â”‚  4.62%  â”‚ R$1.15  â”‚  âœ… < 5%!               â”‚
â”‚  Hidden 64        â”‚  7.30%  â”‚ R$1.93  â”‚  âŒ Pior                â”‚
â”‚  Hidden 100       â”‚  3.73%  â”‚ R$0.85  â”‚  âœ… MELHOR!             â”‚
â”‚  Dropout 0.1      â”‚  4.61%  â”‚ R$1.10  â”‚  âœ… < 5%!               â”‚
â”‚  Dropout 0.3      â”‚  8.04%  â”‚ R$2.10  â”‚  âŒ Muito dropout       â”‚
â”‚  Combo 1          â”‚  4.08%  â”‚ R$0.93  â”‚  âœ… < 5%!               â”‚
â”‚  Combo 2          â”‚  3.79%  â”‚ R$0.86  â”‚  âœ… < 5%!               â”‚
â”‚  Combo 3          â”‚  6.04%  â”‚ R$1.44  â”‚  âŒ NÃ£o funcionou       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.3 AnÃ¡lise dos Experimentos

#### Learning Rate

```
LR 0.001 (padrÃ£o)  â†’ MAPE 5.45%
LR 0.0005          â†’ MAPE 4.34% âœ… Melhorou!
LR 0.0001          â†’ MAPE 51.93% âŒ Muito lento, nÃ£o convergiu

ConclusÃ£o: LR menor que 0.001 ajuda, mas nÃ£o muito menor
```

#### Hidden Size

```
Hidden 50 (padrÃ£o) â†’ MAPE 5.45%
Hidden 64          â†’ MAPE 7.30% âŒ Piorou (estranho, pode ser aleatoriedade)
Hidden 100         â†’ MAPE 3.73% âœ…âœ… MUITO MELHOR!

ConclusÃ£o: Dobrar o hidden_size foi a mudanÃ§a mais impactante!
           O modelo precisava de mais capacidade para aprender padrÃµes.
```

#### Epochs

```
100 epochs (padrÃ£o) â†’ MAPE 5.45%
150 epochs          â†’ MAPE 4.41% âœ… Melhorou
200 epochs          â†’ MAPE 4.62% âœ… Melhorou, mas menos que 150

ConclusÃ£o: Mais Ã©pocas ajudam atÃ© certo ponto
           Depois o ganho diminui (retornos decrescentes)
```

#### Dropout

```
Dropout 0.2 (padrÃ£o) â†’ MAPE 5.45%
Dropout 0.1          â†’ MAPE 4.61% âœ… Melhorou um pouco
Dropout 0.3          â†’ MAPE 8.04% âŒ Piorou muito

ConclusÃ£o: 0.2 Ã© um bom equilÃ­brio
           0.3 "desliga" neurÃ´nios demais, o modelo nÃ£o aprende
```

### 6.4 Descoberta Principal

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    DESCOBERTA CHAVE                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                  â•‘
â•‘  A mudanÃ§a mais impactante foi aumentar o hidden_size de 50     â•‘
â•‘  para 100!                                                       â•‘
â•‘                                                                  â•‘
â•‘  Por quÃª?                                                        â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€                                                        â•‘
â•‘  â€¢ O modelo original tinha "capacidade" limitada                 â•‘
â•‘  â€¢ 50 neurÃ´nios nÃ£o eram suficientes para capturar todos os     â•‘
â•‘    padrÃµes presentes nos 6 anos de dados                        â•‘
â•‘  â€¢ Dobrar para 100 deu ao modelo mais "espaÃ§o" para aprender    â•‘
â•‘                                                                  â•‘
â•‘  Analogia:                                                       â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â•‘
â•‘  Ã‰ como tentar guardar um armÃ¡rio de roupas em uma mala pequena â•‘
â•‘  NÃ£o cabe! Precisa de uma mala maior.                           â•‘
â•‘                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## 7. Modelo Otimizado - VersÃ£o Final

### 7.1 ConfiguraÃ§Ã£o Final

```python
# Modelo Otimizado (v2)
StockLSTM(
    input_size=1,      # Mantido (1 feature)
    hidden_size=100,   # ALTERADO: 50 â†’ 100 (dobrou!)
    num_layers=2,      # Mantido (2 camadas)
    dropout=0.2        # Mantido (20%)
)

# Treinamento
EPOCHS = 100           # Mantido
LEARNING_RATE = 0.001  # Mantido
```

### 7.2 Contagem de ParÃ¢metros (v2)

```
Modelo v2 (hidden_size=100):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LSTM Camada 1:  4 Ã— (1Ã—100 + 100Ã—100 + 100) = 40.800 parÃ¢metros
LSTM Camada 2:  4 Ã— (100Ã—100 + 100Ã—100 + 100) = 80.800 parÃ¢metros
Linear:         100 Ã— 1 + 1 = 101 parÃ¢metros
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:          ~121.000 parÃ¢metros treinÃ¡veis

ComparaÃ§Ã£o:
â€¢ v1: ~31.000 parÃ¢metros
â€¢ v2: ~121.000 parÃ¢metros
â€¢ Aumento: ~4x mais parÃ¢metros
```

### 7.3 Resultados do Treinamento Final

```
============================================================
ğŸ‹ï¸ TREINAMENTO v2 (hidden_size=100)
============================================================

Progresso:
Epoch [ 10/100] | Train Loss: 0.014841 | Val Loss: 0.205917
Epoch [ 20/100] | Train Loss: 0.013054 | Val Loss: 0.185091
Epoch [ 50/100] | Train Loss: 0.001040 | Val Loss: 0.004649
Epoch [ 70/100] | Train Loss: 0.000830 | Val Loss: 0.001263
Epoch [100/100] | Train Loss: 0.000693 | Val Loss: 0.001367

Resumo:
â€¢ Tempo total: 30.8 segundos
â€¢ Train Loss final: 0.000693
â€¢ Val Loss final: 0.001367
â€¢ Melhor Val Loss: 0.001190 (Ã©poca 68)
```

### 7.4 MÃ©tricas Finais

```
============================================================
ğŸ“Š MÃ‰TRICAS DE AVALIAÃ‡ÃƒO - MODELO v2 (OTIMIZADO)
============================================================

MSE  (Mean Squared Error):     0.7964
RMSE (Root Mean Squared Error): R$ 0.89
MAE  (Mean Absolute Error):     R$ 0.70
MAPE (Mean Absolute % Error):   3.83%

DiagnÃ³stico: EXCELENTE! (MAPE < 5%) âœ…
```

---

## 8. Comparativo de Resultados

### 8.1 Tabela Comparativa

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              COMPARATIVO: MODELO v1 vs v2                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  ParÃ¢metro        â”‚  Modelo v1   â”‚  Modelo v2    â”‚  MudanÃ§a     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  hidden_size      â”‚  50          â”‚  100          â”‚  +100%       â”‚
â”‚  ParÃ¢metros       â”‚  ~31.000     â”‚  ~121.000     â”‚  +290%       â”‚
â”‚  Tempo treino     â”‚  18.4s       â”‚  30.8s        â”‚  +67%        â”‚
â”‚                                                                 â”‚
â”‚  MÃ©trica          â”‚  Modelo v1   â”‚  Modelo v2    â”‚  Melhoria    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  MAPE             â”‚  6.74%       â”‚  3.83%        â”‚  -43%  âœ…    â”‚
â”‚  RMSE             â”‚  R$ 1.43     â”‚  R$ 0.89      â”‚  -38%  âœ…    â”‚
â”‚  MAE              â”‚  R$ 1.15     â”‚  R$ 0.70      â”‚  -39%  âœ…    â”‚
â”‚  MSE              â”‚  2.0468      â”‚  0.7964       â”‚  -61%  âœ…    â”‚
â”‚  Train Loss       â”‚  0.002085    â”‚  0.000693     â”‚  -67%  âœ…    â”‚
â”‚  Val Loss         â”‚  0.003514    â”‚  0.001367     â”‚  -61%  âœ…    â”‚
â”‚                                                                 â”‚
â”‚  Status           â”‚  BOM         â”‚  EXCELENTE    â”‚  â†‘â†‘â†‘        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.2 VisualizaÃ§Ã£o da Melhoria

```
MAPE (Erro Percentual):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
v1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  6.74%  (Bom)
v2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    3.83%  (Excelente!)
    0%                              5%               10%
                                    â†‘
                                Meta (< 5%)

RMSE (Erro em R$):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
v1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  R$ 1.43
v2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    R$ 0.89
    R$ 0                          R$ 1.00           R$ 1.50
                                    â†‘
                                Meta (< R$ 1.00)
```

### 8.3 Custo-BenefÃ­cio

```
O que pagamos (custos):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ +67% tempo de treino (18.4s â†’ 30.8s)
â€¢ +290% parÃ¢metros (~31k â†’ ~121k)
â€¢ ~4x mais memÃ³ria do modelo

O que ganhamos (benefÃ­cios):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ -43% no erro percentual (6.74% â†’ 3.83%)
â€¢ -38% no erro em R$ (R$ 1.43 â†’ R$ 0.89)
â€¢ Status "BOM" â†’ "EXCELENTE"
â€¢ MAPE dentro da meta (< 5%)

ConclusÃ£o: VALE A PENA!
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
O aumento no tempo de treino Ã© negligenciÃ¡vel (12 segundos a mais)
O aumento na memÃ³ria Ã© pequeno (~90KB a mais no arquivo .pth)
A melhoria na precisÃ£o Ã© substancial (43% menos erro)
```

---

## 9. LiÃ§Ãµes Aprendidas

### 9.1 Sobre HiperparÃ¢metros

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LIÃ‡Ã•ES APRENDIDAS                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. HIDDEN_SIZE Ã‰ CRUCIAL                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
â”‚  â€¢ ComeÃ§ar pequeno e aumentar se necessÃ¡rio                     â”‚
â”‚  â€¢ Hidden_size muito pequeno = underfitting (nÃ£o aprende)       â”‚
â”‚  â€¢ Hidden_size muito grande = overfitting (decora) + lento      â”‚
â”‚  â€¢ No nosso caso, 50 era pequeno demais para 6 anos de dados    â”‚
â”‚                                                                 â”‚
â”‚  2. LEARNING RATE TEM LIMITE                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
â”‚  â€¢ 0.001 Ã© um bom ponto de partida para Adam                    â”‚
â”‚  â€¢ Muito baixo (0.0001) = nÃ£o converge em 100 Ã©pocas            â”‚
â”‚  â€¢ Muito alto (0.01) = instÃ¡vel, loss oscila                    â”‚
â”‚                                                                 â”‚
â”‚  3. DROPOUT TEM PONTO Ã“TIMO                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                     â”‚
â”‚  â€¢ 0.2 Ã© um bom padrÃ£o                                          â”‚
â”‚  â€¢ 0.3+ desliga neurÃ´nios demais, prejudica aprendizado         â”‚
â”‚  â€¢ 0.1 pode ser melhor em alguns casos                          â”‚
â”‚                                                                 â”‚
â”‚  4. MAIS Ã‰POCAS â‰  SEMPRE MELHOR                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                â”‚
â”‚  â€¢ Retornos decrescentes apÃ³s certo ponto                       â”‚
â”‚  â€¢ 150 Ã©pocas foi melhor que 200 em alguns experimentos         â”‚
â”‚  â€¢ Monitorar val_loss para saber quando parar                   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.2 Sobre o Processo de OtimizaÃ§Ã£o

```
Abordagem que funcionou:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. ComeÃ§ar com valores padrÃ£o da literatura
2. Treinar e avaliar (estabelecer baseline)
3. Variar UM parÃ¢metro por vez
4. Identificar o parÃ¢metro mais impactante
5. Focar nesse parÃ¢metro
6. Testar combinaÃ§Ãµes promissoras

O que NÃƒO funcionou:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Mudar muitos parÃ¢metros de uma vez (difÃ­cil saber o que ajudou)
â€¢ Learning rate muito baixo (modelo nÃ£o aprendeu)
â€¢ Dropout muito alto (modelo "esqueceu" demais)
```

### 9.3 RecomendaÃ§Ãµes para Projetos Futuros

```
Se for fazer um projeto similar:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Reserve tempo para otimizaÃ§Ã£o (nÃ£o Ã© desperdÃ­cio!)
2. Documente cada experimento (vocÃª vai esquecer)
3. Use um hidden_size maior se tiver muitos dados
4. Adam com lr=0.001 Ã© um bom ponto de partida
5. Dropout 0.2 funciona bem para a maioria dos casos
6. Monitore SEMPRE train_loss E val_loss
```

---

## 10. GlossÃ¡rio

| Termo | DefiniÃ§Ã£o |
|-------|-----------|
| **LSTM** | Long Short-Term Memory - Tipo de rede neural que mantÃ©m memÃ³ria de longo prazo |
| **Ã‰poca (Epoch)** | Uma passagem completa por todos os dados de treino |
| **Loss** | FunÃ§Ã£o que mede o erro do modelo (quanto menor, melhor) |
| **MSE** | Mean Squared Error - MÃ©dia dos erros ao quadrado |
| **RMSE** | Root Mean Squared Error - Raiz do MSE (em R$) |
| **MAE** | Mean Absolute Error - MÃ©dia dos erros absolutos |
| **MAPE** | Mean Absolute Percentage Error - Erro percentual mÃ©dio |
| **Learning Rate** | Taxa de aprendizado - Tamanho do "passo" na atualizaÃ§Ã£o dos pesos |
| **Hidden Size** | NÃºmero de neurÃ´nios na camada oculta da LSTM |
| **Dropout** | TÃ©cnica que desliga neurÃ´nios aleatoriamente para evitar overfitting |
| **Overfitting** | Quando o modelo "decora" os dados ao invÃ©s de aprender padrÃµes |
| **Underfitting** | Quando o modelo Ã© simples demais para aprender os padrÃµes |
| **Gradiente** | DireÃ§Ã£o e magnitude da mudanÃ§a necessÃ¡ria em cada peso |
| **Backpropagation** | Algoritmo que calcula gradientes de trÃ¡s para frente |
| **Adam** | Otimizador adaptativo que ajusta learning rate por parÃ¢metro |
| **NormalizaÃ§Ã£o** | Transformar dados para escala 0-1 (facilita o treinamento) |
| **Scaler** | Objeto que guarda parÃ¢metros de normalizaÃ§Ã£o para reverter depois |
| **Tensor** | Estrutura de dados otimizada para operaÃ§Ãµes matriciais |
| **Forward Pass** | Dados atravessando o modelo da entrada para a saÃ­da |
| **Backward Pass** | CÃ¡lculo de gradientes da saÃ­da de volta para a entrada |

---

## Arquivos do Projeto

```
tech-challenge-lstm/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_collection.py      # Coleta de dados (yfinance)
â”‚   â”œâ”€â”€ preprocessing.py        # NormalizaÃ§Ã£o e janelas
â”‚   â”œâ”€â”€ model.py                # Arquitetura LSTM
â”‚   â”œâ”€â”€ train.py                # Loop de treinamento
â”‚   â”œâ”€â”€ evaluate.py             # CÃ¡lculo de mÃ©tricas
â”‚   â””â”€â”€ hyperparameter_tuning.py # Experimentos de otimizaÃ§Ã£o
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ model_lstm.pth          # Modelo treinado (v2 - otimizado)
â”‚   â”œâ”€â”€ scaler.pkl              # Normalizador
â”‚   â”œâ”€â”€ config.pkl              # ConfiguraÃ§Ãµes
â”‚   â”œâ”€â”€ training_history.png    # GrÃ¡fico de loss
â”‚   â””â”€â”€ predictions_vs_actual.png # GrÃ¡fico de previsÃµes
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ data_PETR4_SA.csv       # Dados histÃ³ricos
â”‚
â””â”€â”€ docs/
    â””â”€â”€ CICLO_VIDA_TREINAMENTO.md  # Este documento
```

---

**ConclusÃ£o Final:**

O modelo evoluiu de uma performance "Boa" para "Excelente" atravÃ©s de um processo sistemÃ¡tico de otimizaÃ§Ã£o. A descoberta principal foi que o hidden_size inicial de 50 era insuficiente para a quantidade de dados disponÃ­veis. Ao dobrar para 100, o modelo ganhou capacidade suficiente para capturar padrÃµes mais complexos nos 6 anos de dados histÃ³ricos, resultando em uma reduÃ§Ã£o de 43% no erro percentual.

---

*Documento criado em 19/02/2026*
