OlÃ¡! Sou seu tutor especializado e preparei este **Guia PrÃ¡tico de ExecuÃ§Ã£o do Tech Challenge - Fase 4**. Meu objetivo Ã© transformar os conceitos densos das aulas em passos palpÃ¡veis para vocÃª construir seu preditor de aÃ§Ãµes com **LSTM**, garantindo estrutura e foco para sua jornada.

---

## ğŸ¨ PARTE 1: MAPA DO TESOURO ğŸ—ºï¸

Este projeto Ã© uma jornada linear. NÃ£o pule etapas!

```text
INÃCIO: Setup do Ambiente (30 min) ğŸ› ï¸
  â†“
COLETA: Dados HistÃ³ricos yfinance (30 min) ğŸ“¥ â” ğŸ‰ Checkpoint: "Dados na mÃ£o!"
  â†“
PRÃ‰-PROCESSAMENTO: NormalizaÃ§Ã£o e Janelas (45 min) ğŸ§¹ â” ğŸ‰ Checkpoint: "Dados prontos!"
  â†“
MODELAGEM: ConstruÃ§Ã£o da Rede LSTM (45 min) ğŸ§  â” ğŸ‰ Checkpoint: "O cÃ©rebro nasceu!"
  â†“
TREINAMENTO: Ajuste de HiperparÃ¢metros (1h+) â³
  â†“
AVALIAÃ‡ÃƒO: MÃ©tricas MAE/RMSE/MAPE (30 min) ğŸ“ â” ğŸ‰ Checkpoint: "Modelo validado!"
  â†“
SALVAMENTO: Exportar modelo treinado (15 min) ğŸ’¾
  â†“
DEPLOY: CriaÃ§Ã£o da API e Docker (1h) ğŸš€
  â†“
FINAL: Monitoramento e DocumentaÃ§Ã£o (45 min) ğŸ“‹ â” ğŸ† RECOMPENSA FINAL!
```

**DependÃªncias:** VocÃª nÃ£o pode treinar sem prÃ©-processar, e nÃ£o pode fazer o deploy sem o modelo salvo.

---

## ğŸ§ª PARTE 2: CONEXÃƒO TEORIA â†” PRÃTICA

| Conceito do Tech Challenge | Onde estÃ¡ nas Aulas |
|---------------------------|---------------------|
| **LSTM e RNNs** | Aula 3, Etapa 1 - Arquiteturas de Redes Neurais (seÃ§Ã£o RNNs) |
| **Backpropagation Through Time** | Aula 3, Etapa 1 - "BPTT desenrola a rede no tempo" |
| **Dropout e RegularizaÃ§Ã£o** | Aula 3, Etapa 1 - Evitar overfitting |
| **Otimizador Adam** | Aula 3 e 4, Etapa 1 - TÃ©cnicas de otimizaÃ§Ã£o |
| **SÃ©ries Temporais** | Aula 3, Etapa 1 - "RNNs para processamento de sequÃªncias" |
| **Deploy e ProduÃ§Ã£o** | Aula 4 e 5, Etapa 1 - IntegraÃ§Ãµes Transacionais |

> **Nota:** A LSTM Ã© abordada principalmente na **Etapa 1 (Deep Learning)**, na seÃ§Ã£o sobre RNNs. A Etapa 4 (Generative AI) foca em Transformers/Attention, que sÃ£o uma evoluÃ§Ã£o das RNNs.

---

## ğŸš€ PARTE 3: GUIA PASSO A PASSO COMPLETO

### ğŸ“Œ ETAPA 1: ConfiguraÃ§Ã£o do Ambiente
**ğŸ“ CONCEITOS DAS AULAS:**
*   **Bibliotecas de Deep Learning:** Como vimos na Aula 3 da Etapa 1, utilizaremos o **PyTorch** (ou TensorFlow) para gerenciar tensores e camadas.

**ğŸ¯ OBJETIVO:** Ter uma pasta organizada e as bibliotecas instaladas.

**ğŸ“‹ CHECKLIST:**
- [ ] Criar ambiente virtual (`venv`)
- [ ] Instalar dependÃªncias

**ğŸ’» CÃ“DIGO:**
```bash
# Criar e ativar ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou: venv\Scripts\activate  # Windows

# Instalar dependÃªncias
pip install yfinance pandas numpy torch scikit-learn fastapi uvicorn matplotlib
```

**ğŸ“„ requirements.txt:**
```text
yfinance>=0.2.0
pandas>=2.0.0
numpy>=1.24.0
torch>=2.0.0
scikit-learn>=1.3.0
fastapi>=0.100.0
uvicorn>=0.23.0
matplotlib>=3.7.0
```

---

### ğŸ“Œ ETAPA 2: Coleta de Dados (yfinance)
**ğŸ“ CONCEITOS DAS AULAS:**
*   **Dados Estruturados:** Como visto na Aula 4 da Etapa 1, preÃ§os de aÃ§Ãµes sÃ£o dados tabulares organizados que alimentam redes neurais para tarefas de regressÃ£o.

**ğŸ¯ OBJETIVO:** Baixar o histÃ³rico de fechamento de uma empresa (ex: 'AAPL' ou 'PETR4.SA').

**ğŸ’» CÃ“DIGO BASE COMENTADO:**
```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ ETAPA 2: Coleta de Dados
# ğŸ¯ Objetivo: Baixar preÃ§os histÃ³ricos usando a biblioteca yfinance
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import yfinance as yf
import pandas as pd

# 1ï¸âƒ£ Definir o ativo e o perÃ­odo
# Por que: Precisamos de uma janela temporal longa para a LSTM aprender padrÃµes
ticker = "PETR4.SA"  # Ou "AAPL", "MSFT", etc.
start_date = "2018-01-01"
end_date = "2024-01-01"

# 2ï¸âƒ£ Baixar os dados
df = yf.download(ticker, start=start_date, end=end_date)

# 3ï¸âƒ£ Verificar os dados
print(f"Shape: {df.shape}")
print(df.head())
print(df.tail())

# 4ï¸âƒ£ Salvar para uso posterior (opcional)
df.to_csv(f"data_{ticker}.csv")

# âœ… Checkpoint: Se df.head() mostrar preÃ§os, vocÃª tem os dados!
```

---

### ğŸ“Œ ETAPA 3: PrÃ©-processamento para SÃ©ries Temporais
**ğŸ“ CONCEITOS DAS AULAS:**
*   **NormalizaÃ§Ã£o:** Essencial para evitar que valores grandes dominem o cÃ¡lculo do erro (Aula 2, Etapa 1).
*   **Janelas Deslizantes:** Como as RNNs processam sequÃªncias, precisamos "fatiar" os dados em blocos temporais (Aula 3, Etapa 1 - seÃ§Ã£o RNNs).

**ğŸ¯ OBJETIVO:** Escalar os dados entre 0 e 1 e criar sequÃªncias de X dias para prever o dia X+1.

**ğŸ’» CÃ“DIGO COMPLETO:**
```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ ETAPA 3: PrÃ©-processamento
# ğŸ¯ Objetivo: Normalizar dados e criar janelas temporais
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import numpy as np
from sklearn.preprocessing import MinMaxScaler

# 1ï¸âƒ£ Selecionar apenas a coluna 'Close' (preÃ§o de fechamento)
data = df['Close'].values.reshape(-1, 1)

# 2ï¸âƒ£ Normalizar os dados entre 0 e 1
# Por que: Redes neurais funcionam melhor com valores pequenos e uniformes
scaler = MinMaxScaler(feature_range=(0, 1))
data_scaled = scaler.fit_transform(data)

# 3ï¸âƒ£ Criar janelas deslizantes (sequÃªncias)
def create_sequences(data, seq_length):
    """
    Cria sequÃªncias de entrada (X) e saÃ­da (y) para a LSTM.
    X: Ãºltimos `seq_length` dias
    y: dia seguinte (o que queremos prever)
    """
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i+seq_length])
        y.append(data[i+seq_length])
    return np.array(X), np.array(y)

# 4ï¸âƒ£ Definir o tamanho da janela (60 dias = ~3 meses de histÃ³rico)
SEQ_LENGTH = 60
X, y = create_sequences(data_scaled, SEQ_LENGTH)

# 5ï¸âƒ£ Dividir em treino (80%) e teste (20%)
split = int(len(X) * 0.8)
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# 6ï¸âƒ£ Converter para tensores PyTorch
import torch

X_train = torch.FloatTensor(X_train)
y_train = torch.FloatTensor(y_train)
X_test = torch.FloatTensor(X_test)
y_test = torch.FloatTensor(y_test)

print(f"X_train shape: {X_train.shape}")  # (amostras, seq_length, features)
print(f"y_train shape: {y_train.shape}")  # (amostras, 1)

# âœ… Checkpoint: Dados prontos para alimentar a LSTM!
```

---

### ğŸ“Œ ETAPA 4: ConstruÃ§Ã£o do Modelo LSTM
**ğŸ“ CONCEITOS DAS AULAS:**
*   **LSTM (Long Short-Term Memory):** Diferente das RNNs comuns, a LSTM possui "portÃµes" (gates) que decidem o que manter na memÃ³ria de longo prazo, evitando o desaparecimento do gradiente (Aula 3, Etapa 1 - seÃ§Ã£o RNNs/LSTM).
*   **Dropout:** Camada para evitar o **Overfitting**, desligando neurÃ´nios aleatoriamente (Aula 3, Etapa 1).

**ğŸ’» CÃ“DIGO BASE COMENTADO:**
```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ ETAPA 4: Modelo LSTM
# ğŸ¯ Objetivo: Definir a arquitetura da rede neural
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import torch.nn as nn

class StockLSTM(nn.Module):
    def __init__(self, input_size=1, hidden_size=50, num_layers=2, dropout=0.2):
        super(StockLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # 1ï¸âƒ£ Camada LSTM: O coraÃ§Ã£o que guarda o contexto temporal
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # 2ï¸âƒ£ Dropout: Evita que o modelo "decore" os preÃ§os passados (overfitting)
        self.dropout = nn.Dropout(dropout)
        
        # 3ï¸âƒ£ Camada Linear: Transforma a memÃ³ria da LSTM no preÃ§o final previsto
        self.linear = nn.Linear(hidden_size, 1)

    def forward(self, x):
        # x shape: (batch_size, seq_length, input_size)
        lstm_out, (h_n, c_n) = self.lstm(x)
        
        # Pega apenas o Ãºltimo passo da sequÃªncia
        last_output = lstm_out[:, -1, :]
        
        # Aplica dropout e camada linear
        out = self.dropout(last_output)
        prediction = self.linear(out)
        
        return prediction

# Instanciar o modelo
model = StockLSTM(input_size=1, hidden_size=50, num_layers=2, dropout=0.2)
print(model)

# âœ… Checkpoint: Modelo instanciado sem erros!
```

---

### ğŸ“Œ ETAPA 5: Treinamento e Ajuste
**ğŸ“ CONCEITOS DAS AULAS:**
*   **Backpropagation Through Time (BPTT):** O erro volta ajustando os pesos ao longo do tempo (Aula 3, Etapa 1).
*   **Otimizador Adam:** Ajusta a taxa de aprendizado de forma adaptativa (Aula 3 e 4, Etapa 1).
*   **FunÃ§Ã£o de Perda MSE:** Para tarefas de regressÃ£o (Aula 2, Etapa 1).

**ğŸ’» CÃ“DIGO COMPLETO:**
```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ ETAPA 5: Treinamento
# ğŸ¯ Objetivo: Treinar o modelo ajustando os pesos
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import torch.optim as optim

# 1ï¸âƒ£ Configurar dispositivo (GPU se disponÃ­vel)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# 2ï¸âƒ£ Definir funÃ§Ã£o de perda e otimizador
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 3ï¸âƒ£ Mover dados para o dispositivo
X_train = X_train.to(device)
y_train = y_train.to(device)
X_test = X_test.to(device)
y_test = y_test.to(device)

# 4ï¸âƒ£ Loop de treinamento
EPOCHS = 100
train_losses = []
val_losses = []

for epoch in range(EPOCHS):
    model.train()
    
    # Forward pass
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    
    # Backward pass
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # ValidaÃ§Ã£o
    model.eval()
    with torch.no_grad():
        val_outputs = model(X_test)
        val_loss = criterion(val_outputs, y_test)
    
    train_losses.append(loss.item())
    val_losses.append(val_loss.item())
    
    # Imprimir a cada 10 Ã©pocas
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {loss.item():.6f}, Val Loss: {val_loss.item():.6f}')

# âœ… Checkpoint: Modelo treinado!
```

**âš ï¸ ARMADILHAS:** 
*   **Perda estagnada:** Taxa de aprendizado muito alta. Tente `lr=0.0001`.
*   **Overfitting:** Val Loss sobe enquanto Train Loss desce. Aumente dropout ou reduza Ã©pocas.

---

### ğŸ“Œ ETAPA 6: AvaliaÃ§Ã£o do Modelo
**ğŸ“ CONCEITOS DAS AULAS:**
*   **MÃ©tricas de RegressÃ£o:** MAE, RMSE, MAPE sÃ£o mÃ©tricas padrÃ£o para avaliar previsÃµes numÃ©ricas (Aula 4, Etapa 1).

**ğŸ¯ OBJETIVO:** Calcular mÃ©tricas e visualizar previsÃµes vs valores reais.

**ğŸ’» CÃ“DIGO COMPLETO:**
```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ ETAPA 6: AvaliaÃ§Ã£o
# ğŸ¯ Objetivo: Calcular mÃ©tricas MAE, RMSE, MAPE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error

# 1ï¸âƒ£ Fazer previsÃµes no conjunto de teste
model.eval()
with torch.no_grad():
    predictions = model(X_test).cpu().numpy()
    actuals = y_test.cpu().numpy()

# 2ï¸âƒ£ Reverter a normalizaÃ§Ã£o (para valores em R$)
predictions_inv = scaler.inverse_transform(predictions)
actuals_inv = scaler.inverse_transform(actuals)

# 3ï¸âƒ£ Calcular mÃ©tricas
mae = mean_absolute_error(actuals_inv, predictions_inv)
rmse = np.sqrt(mean_squared_error(actuals_inv, predictions_inv))
mape = np.mean(np.abs((actuals_inv - predictions_inv) / actuals_inv)) * 100

print(f"ğŸ“Š MÃ‰TRICAS DE AVALIAÃ‡ÃƒO:")
print(f"   MAE  (Erro MÃ©dio Absoluto): R$ {mae:.2f}")
print(f"   RMSE (Raiz do Erro QuadrÃ¡tico MÃ©dio): R$ {rmse:.2f}")
print(f"   MAPE (Erro Percentual MÃ©dio): {mape:.2f}%")

# 4ï¸âƒ£ Visualizar previsÃµes vs valores reais
plt.figure(figsize=(14, 5))
plt.plot(actuals_inv, label='Valores Reais', color='blue')
plt.plot(predictions_inv, label='PrevisÃµes', color='red', alpha=0.7)
plt.title(f'PrevisÃ£o de PreÃ§os - {ticker}')
plt.xlabel('Dias')
plt.ylabel('PreÃ§o (R$)')
plt.legend()
plt.savefig('prediction_chart.png')
plt.show()

# âœ… Checkpoint: Modelo avaliado com mÃ©tricas!
```

---

### ğŸ“Œ ETAPA 7: Salvamento do Modelo
**ğŸ¯ OBJETIVO:** Salvar o modelo treinado para uso posterior na API.

**ğŸ’» CÃ“DIGO:**
```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ ETAPA 7: Salvamento
# ğŸ¯ Objetivo: Exportar modelo e scaler para deploy
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import joblib

# 1ï¸âƒ£ Salvar o modelo PyTorch
torch.save({
    'model_state_dict': model.state_dict(),
    'model_config': {
        'input_size': 1,
        'hidden_size': 50,
        'num_layers': 2,
        'dropout': 0.2
    }
}, 'model_lstm.pth')

# 2ï¸âƒ£ Salvar o scaler (necessÃ¡rio para normalizar novos dados)
joblib.dump(scaler, 'scaler.pkl')

# 3ï¸âƒ£ Salvar configuraÃ§Ãµes
config = {
    'seq_length': SEQ_LENGTH,
    'ticker': ticker
}
joblib.dump(config, 'config.pkl')

print("âœ… Modelo, scaler e config salvos!")
```

---

### ğŸ“Œ ETAPA 8: CriaÃ§Ã£o da API (FastAPI)
**ğŸ“ CONCEITOS DAS AULAS:**
*   **IntegraÃ§Ãµes Transacionais:** Como visto na Aula 5 da Etapa 1, modelos em produÃ§Ã£o precisam de alta disponibilidade e interfaces claras para o usuÃ¡rio.

**ğŸ¯ OBJETIVO:** Criar um endpoint `/predict` que receba os Ãºltimos preÃ§os e retorne o valor futuro.

**ğŸ’» CÃ“DIGO COMPLETO (app.py):**
```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ ETAPA 8: API FastAPI
# ğŸ¯ Objetivo: Servir o modelo via endpoint REST
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
import joblib
import numpy as np
from typing import List
import time

app = FastAPI(title="Stock Price Predictor API", version="1.0.0")

# Carregar modelo e configuraÃ§Ãµes no startup
model = None
scaler = None
config = None

@app.on_event("startup")
def load_model():
    global model, scaler, config
    
    # Carregar config
    config = joblib.load('config.pkl')
    scaler = joblib.load('scaler.pkl')
    
    # Carregar modelo
    checkpoint = torch.load('model_lstm.pth', map_location='cpu')
    model_config = checkpoint['model_config']
    
    model = StockLSTM(**model_config)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print("âœ… Modelo carregado com sucesso!")

# Schema de entrada
class PredictionRequest(BaseModel):
    prices: List[float]  # Lista com os Ãºltimos N preÃ§os de fechamento

# Schema de saÃ­da
class PredictionResponse(BaseModel):
    predicted_price: float
    confidence_info: str
    processing_time_ms: float

@app.post("/predict", response_model=PredictionResponse)
def predict(request: PredictionRequest):
    start_time = time.time()
    
    # Validar entrada
    if len(request.prices) < config['seq_length']:
        raise HTTPException(
            status_code=400, 
            detail=f"NecessÃ¡rio pelo menos {config['seq_length']} preÃ§os histÃ³ricos"
        )
    
    # Pegar os Ãºltimos seq_length preÃ§os
    prices = np.array(request.prices[-config['seq_length']:]).reshape(-1, 1)
    
    # Normalizar
    prices_scaled = scaler.transform(prices)
    
    # Converter para tensor
    X = torch.FloatTensor(prices_scaled).unsqueeze(0)  # (1, seq_length, 1)
    
    # Fazer previsÃ£o
    with torch.no_grad():
        prediction_scaled = model(X).numpy()
    
    # Reverter normalizaÃ§Ã£o
    predicted_price = scaler.inverse_transform(prediction_scaled)[0][0]
    
    processing_time = (time.time() - start_time) * 1000
    
    return PredictionResponse(
        predicted_price=float(predicted_price),
        confidence_info="PrevisÃ£o baseada em modelo LSTM",
        processing_time_ms=round(processing_time, 2)
    )

@app.get("/health")
def health_check():
    return {"status": "healthy", "model_loaded": model is not None}

# Para rodar: uvicorn app:app --reload
```

---

### ğŸ“Œ ETAPA 9: Docker e Monitoramento
**ğŸ¯ OBJETIVO:** Containerizar a aplicaÃ§Ã£o e configurar monitoramento bÃ¡sico.

**ğŸ“„ Dockerfile:**
```dockerfile
FROM python:3.10-slim

WORKDIR /app

# Copiar requirements e instalar dependÃªncias
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copiar arquivos do projeto
COPY app.py .
COPY model_lstm.pth .
COPY scaler.pkl .
COPY config.pkl .

# Expor porta
EXPOSE 8000

# Comando para rodar a API
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
```

**ğŸ“„ docker-compose.yml (opcional):**
```yaml
version: '3.8'
services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - PYTHONUNBUFFERED=1
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

**ğŸ”§ Comandos para rodar:**
```bash
# Construir imagem
docker build -t stock-predictor-api .

# Rodar container
docker run -p 8000:8000 stock-predictor-api

# Testar API
curl http://localhost:8000/health
```

**ğŸ“Š Monitoramento BÃ¡sico:**
- O endpoint `/health` jÃ¡ fornece status do modelo
- O campo `processing_time_ms` na resposta mede tempo de inferÃªncia
- Logs do uvicorn mostram requisiÃ§Ãµes e erros

---

## ğŸ¨ PARTE 4: RECURSOS DE APOIO

### ğŸ“– GLOSSÃRIO DO PROJETO
*   **Epoch (Ã‰poca):** Uma passagem completa por todo o dataset de treino.
*   **Hidden State:** A "memÃ³ria" interna que a LSTM carrega entre os dias.
*   **MAE (Mean Absolute Error):** MÃ©dia de quÃ£o longe (em R$) sua previsÃ£o estÃ¡ da realidade.
*   **RMSE:** Raiz do erro quadrÃ¡tico mÃ©dio - penaliza mais erros grandes.
*   **MAPE:** Erro percentual mÃ©dio - Ãºtil para comparar entre ativos diferentes.
*   **Scaler:** Objeto que guarda os parÃ¢metros de normalizaÃ§Ã£o para reverter depois.

### ğŸ“… CRONOGRAMA SUGERIDO (PAUSAS TDAH)
1.  **SessÃ£o 1 (45 min):** Etapas 1-2 (Setup e Coleta). **PAUSA: 10 min (CafÃ©)**.
2.  **SessÃ£o 2 (45 min):** Etapa 3 (PrÃ©-processamento). **PAUSA: 15 min (Caminhada)**.
3.  **SessÃ£o 3 (45 min):** Etapas 4-5 (Modelo e Treino). **PAUSA: 10 min**.
4.  **SessÃ£o 4 (45 min):** Etapas 6-7 (AvaliaÃ§Ã£o e Salvamento). **PAUSA: 10 min**.
5.  **SessÃ£o 5 (60 min):** Etapas 8-9 (API e Docker). **RECOMPENSA: EpisÃ³dio de sÃ©rie!** ğŸ†
6.  **SessÃ£o 6 (30 min):** DocumentaÃ§Ã£o e VÃ­deo.

### â“ FAQ DO PROJETO
*   **"Meu modelo estÃ¡ prevendo sempre o mesmo valor":** Verifique se vocÃª normalizou os dados e se a janela temporal nÃ£o Ã© pequena demais (tente 60 dias).
*   **"Por que LSTM e nÃ£o RNN comum?":** Porque as RNNs "esquecem" muito rÃ¡pido. A LSTM resolve isso com sua estrutura de cÃ©lulas de memÃ³ria (gates).
*   **"Qual mÃ©trica Ã© mais importante?":** MAPE Ã© mais intuitivo (erro em %), mas MAE em R$ Ã© mais tangÃ­vel para o usuÃ¡rio final.
*   **"Posso usar TensorFlow em vez de PyTorch?":** Sim! A lÃ³gica Ã© a mesma, apenas a sintaxe muda.

### ğŸ“‹ CHECKLIST DE ENTREGA
- [ ] CÃ³digo-fonte no repositÃ³rio Git
- [ ] requirements.txt com versÃµes
- [ ] README.md documentando o projeto
- [ ] Modelo treinado (.pth) e scaler (.pkl)
- [ ] Dockerfile funcional
- [ ] MÃ©tricas de avaliaÃ§Ã£o calculadas
- [ ] VÃ­deo demonstrando a API funcionando

**Dica Final de TDAH:** Use o **Modo Foco** no celular e deixe um timer visÃ­vel na mesa. Cada linha de cÃ³digo comentada Ã© um pequeno passo para o sucesso! ğŸš€âœ…