# ğŸ“Œ ETAPA 5: Treinamento

## ğŸ“‹ Resumo
| Item | Valor |
|------|-------|
| **Status** | âœ… ConcluÃ­da + Otimizada |
| **Data Inicial** | 2026-02-17 |
| **Data OtimizaÃ§Ã£o** | 2026-02-19 |
| **Tempo Estimado** | 45 min |
| **Tempo Real** | ~15 min (inicial) + ~30 min (otimizaÃ§Ã£o) |

---

## ğŸ¯ Objetivo
Treinar o modelo LSTM ajustando os pesos atravÃ©s do algoritmo de backpropagation, monitorando as mÃ©tricas de treino e validaÃ§Ã£o.

---

## ğŸ“ ConexÃ£o com as Aulas

### Aula 02 - Teoria das Redes Neurais Profundas
**Conceitos fundamentais aplicados:**

#### Backpropagation e Gradiente Descendente
> *"A eficiÃªncia e a eficÃ¡cia do mÃ©todo de backpropagation sÃ£o amplamente impactadas pela escolha do algoritmo de otimizaÃ§Ã£o, como SGD, Adam ou RMSprop."* (Aula 03, linha ~330)

### Aula 03 - Arquiteturas de Redes Neurais Profundas
**Arquivo:** `docs - fase 4 /etapa 1 - redes neurais e deep learning/Aula 03 - Arquiteturas de Redes Neurais Profundas.txt`

#### Backpropagation Through Time (BPTT)
> *"Do ponto de vista da otimizaÃ§Ã£o, RNNs sÃ£o geralmente treinadas usando variantes do algoritmo de backpropagation chamado Backpropagation Through Time (BPTT). O BPTT desenrola a rede no tempo e aplica o algoritmo de gradiente descendente."* (Linha ~444)

#### Otimizador Adam
> *"Em termos de otimizaÃ§Ã£o, as RNNs frequentemente utilizam tÃ©cnicas como a normalizaÃ§Ã£o de gradientes ou o uso de algoritmos de otimizaÃ§Ã£o robustos, como RMSprop ou Adam, que sÃ£o mais eficazes em lidar com as rÃ¡pidas mudanÃ§as nos gradientes."* (Linha ~506)

#### FunÃ§Ã£o de Perda (MSE)
> *"A atualizaÃ§Ã£o dos pesos w em cada camada, utilizando o gradiente descendente, Ã© dada por: w_{n+1} = w_n - Î·(âˆ‚L/âˆ‚w)"* (Linha ~362-364)

---

## ğŸ“ Arquivo Implementado

### `src/train.py`

#### Estrutura do CÃ³digo

```python
# Linhas 1-5: CabeÃ§alho
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ ETAPA 5: Treinamento
# ğŸ¯ Objetivo: Treinar o modelo ajustando os pesos
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

#### ConfiguraÃ§Ãµes (Linhas 22-29)
```python
EPOCHS = 100          # NÃºmero de iteraÃ§Ãµes completas pelo dataset
LEARNING_RATE = 0.001 # Taxa de aprendizado (quÃ£o rÃ¡pido os pesos mudam)
BATCH_SIZE = None     # None = batch gradient descent (todo dataset)
```

---

## ğŸ”¬ FunÃ§Ã£o Principal: `train_model()` (Linhas 32-153)

### Estrutura do Loop de Treinamento

```python
def train_model(model, X_train, y_train, X_test, y_test, 
                epochs=100, learning_rate=0.001, device=None):
    
    # 1ï¸âƒ£ Configurar dispositivo (GPU/CPU)
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = model.to(device)
    
    # 2ï¸âƒ£ Mover dados para o dispositivo
    X_train = X_train.to(device)
    y_train = y_train.to(device)
    X_test = X_test.to(device)
    y_test = y_test.to(device)
    
    # 3ï¸âƒ£ Definir funÃ§Ã£o de perda e otimizador
    criterion = nn.MSELoss()                          # Mean Squared Error
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    
    # 4ï¸âƒ£ Loop de treinamento
    for epoch in range(epochs):
        # FASE DE TREINO
        model.train()
        outputs = model(X_train)           # Forward pass
        loss = criterion(outputs, y_train) # Calcular erro
        
        optimizer.zero_grad()              # Limpar gradientes
        loss.backward()                    # Backpropagation
        optimizer.step()                   # Atualizar pesos
        
        # FASE DE VALIDAÃ‡ÃƒO
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_test)
            val_loss = criterion(val_outputs, y_test)
    
    return model, train_losses, val_losses
```

---

## ğŸ“Š ConexÃ£o CÃ³digo â†” Teoria

### Tabela de Mapeamento

| Conceito na Aula | Linha na Aula | CÃ³digo | Linha no CÃ³digo |
|------------------|---------------|--------|-----------------|
| "Gradiente descendente" | ~330 | `optimizer.step()` | 113 |
| "Backpropagation" | ~330, ~444 | `loss.backward()` | 112 |
| "Adam optimizer" | ~506 | `optim.Adam(...)` | 75 |
| "FunÃ§Ã£o de perda" | ~362 | `nn.MSELoss()` | 72 |
| "Taxa de aprendizado Î·" | ~364 | `lr=learning_rate` | 75 |

### MSELoss - Por que escolhemos?

> *"A funÃ§Ã£o de perda MSE (Mean Squared Error) Ã© ideal para problemas de regressÃ£o onde queremos minimizar a diferenÃ§a quadrÃ¡tica entre previsÃµes e valores reais."*

```python
# MSE = (1/n) * Î£(y_pred - y_real)Â²
criterion = nn.MSELoss()
loss = criterion(outputs, y_train)
```

### Adam - Por que escolhemos?

> *"Adam: Otimizador adaptativo que ajusta a taxa de aprendizado de cada parÃ¢metro de forma adaptativa."* (Aula 03, linha ~580)

**Vantagens do Adam:**
- Combina vantagens de AdaGrad e RMSprop
- Funciona bem com dados esparsos
- Ajusta automaticamente o learning rate por parÃ¢metro
- Ideal para RNNs/LSTMs

---

## ğŸ”„ Fluxo do Treinamento

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LOOP DE TREINAMENTO                      â”‚
â”‚                      (100 Ã©pocas)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              FASE DE TREINO (model.train())         â”‚   â”‚
â”‚  â”‚                                                     â”‚   â”‚
â”‚  â”‚  1. Forward Pass                                    â”‚   â”‚
â”‚  â”‚     outputs = model(X_train)                        â”‚   â”‚
â”‚  â”‚     â†’ PrevisÃµes do modelo                           â”‚   â”‚
â”‚  â”‚                                                     â”‚   â”‚
â”‚  â”‚  2. Calcular Loss                                   â”‚   â”‚
â”‚  â”‚     loss = MSE(outputs, y_train)                    â”‚   â”‚
â”‚  â”‚     â†’ QuÃ£o errado o modelo estÃ¡                     â”‚   â”‚
â”‚  â”‚                                                     â”‚   â”‚
â”‚  â”‚  3. Backward Pass (Backpropagation)                 â”‚   â”‚
â”‚  â”‚     optimizer.zero_grad()  â†’ Limpa gradientes       â”‚   â”‚
â”‚  â”‚     loss.backward()        â†’ Calcula âˆ‚L/âˆ‚w          â”‚   â”‚
â”‚  â”‚     optimizer.step()       â†’ w = w - Î· * âˆ‚L/âˆ‚w      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                 â”‚
â”‚                           â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚            FASE DE VALIDAÃ‡ÃƒO (model.eval())         â”‚   â”‚
â”‚  â”‚                                                     â”‚   â”‚
â”‚  â”‚  with torch.no_grad():  â†’ NÃ£o calcula gradientes    â”‚   â”‚
â”‚  â”‚      val_outputs = model(X_test)                    â”‚   â”‚
â”‚  â”‚      val_loss = MSE(val_outputs, y_test)            â”‚   â”‚
â”‚  â”‚                                                     â”‚   â”‚
â”‚  â”‚  â†’ Monitora overfitting (se val_loss sobe)          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ˆ Resultados do Treinamento

### VersÃ£o Inicial (v1) - hidden_size=50

```
============================================================
ğŸ‹ï¸ Treinamento v1 (hidden_size=50)
============================================================

Epoch [ 10/100] | Train Loss: 0.027582 | Val Loss: 0.293079
Epoch [ 50/100] | Train Loss: 0.012421 | Val Loss: 0.150984
Epoch [100/100] | Train Loss: 0.002085 | Val Loss: 0.003514

ğŸ“Š Resumo v1:
   Train Loss final: 0.002085
   Val Loss final: 0.003514
   MAPE resultante: 6.74% (Bom, mas abaixo da meta)
```

### VersÃ£o Otimizada (v2) - hidden_size=100 âœ…

ApÃ³s anÃ¡lise de hiperparÃ¢metros (ver `src/hyperparameter_tuning.py`), identificamos que o **hidden_size** era o fator mais impactante.

```
============================================================
ğŸ‹ï¸ Treinamento v2 (hidden_size=100) - MODELO ATUAL
============================================================

Epoch [ 10/100] | Train Loss: 0.014841 | Val Loss: 0.205917
Epoch [ 50/100] | Train Loss: 0.001040 | Val Loss: 0.004649
Epoch [ 70/100] | Train Loss: 0.000830 | Val Loss: 0.001263
Epoch [100/100] | Train Loss: 0.000693 | Val Loss: 0.001367

ğŸ“Š Resumo v2:
   Tempo total: 30.8s
   Train Loss final: 0.000693
   Val Loss final: 0.001367
   Melhor Val Loss: 0.001190 (Ã©poca 68)
   MAPE resultante: 3.83% (EXCELENTE!) âœ…
```

### Comparativo v1 vs v2

| MÃ©trica | v1 (hidden=50) | v2 (hidden=100) | Melhoria |
|---------|----------------|-----------------|----------|
| Train Loss | 0.002085 | 0.000693 | -67% |
| Val Loss | 0.003514 | 0.001367 | -61% |
| MAPE | 6.74% | 3.83% | -43% |
| Status | Bom | **Excelente** | â†‘â†‘â†‘ |

**DiagnÃ³stico:**
- âœ… **Train Loss caindo:** Modelo estÃ¡ aprendendo
- âœ… **Val Loss caindo:** Modelo estÃ¡ generalizando
- âœ… **Val Loss > Train Loss:** Normal, indica que nÃ£o hÃ¡ underfitting
- âœ… **Gap estÃ¡vel:** NÃ£o hÃ¡ sinais graves de overfitting
- âœ… **MAPE < 5%:** Meta atingida!

---

## ğŸ“Š GrÃ¡fico de Treinamento

O arquivo `models/training_history.png` mostra:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HistÃ³rico de Treinamento                                â”‚
â”‚                                                          â”‚
â”‚  Loss â”‚                                                  â”‚
â”‚  0.01 â”‚â•²                                                 â”‚
â”‚       â”‚ â•²                                                â”‚
â”‚       â”‚  â•²   Val Loss (vermelho)                         â”‚
â”‚       â”‚   â•²                                              â”‚
â”‚       â”‚    â•²                                             â”‚
â”‚  0.005â”‚     â•²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚
â”‚       â”‚      â•²                                           â”‚
â”‚       â”‚       â•²  Train Loss (azul)                       â”‚
â”‚       â”‚        â•²                                         â”‚
â”‚  0.001â”‚         â•²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”‚
â”‚       â”‚                                                  â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Ã‰poca   â”‚
â”‚        0    20    40    60    80    100                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¾ Artefatos Salvos

### `models/model_lstm.pth` (Modelo Otimizado v2)

```python
torch.save({
    'model_state_dict': model.state_dict(),    # Pesos treinados
    'model_config': {
        'input_size': 1,
        'hidden_size': 100,    # OTIMIZADO: era 50, agora 100
        'num_layers': 2,
        'dropout': 0.2
    },
    'train_losses': train_losses,              # HistÃ³rico
    'val_losses': val_losses,
    'final_train_loss': 0.000693,
    'final_val_loss': 0.001367
}, 'models/model_lstm.pth')
```

### `src/hyperparameter_tuning.py` (Script de OtimizaÃ§Ã£o)

Script que executa experimentos sistemÃ¡ticos variando:
- Learning rate: [0.0005, 0.001, 0.0001]
- Epochs: [100, 150, 200]
- Hidden size: [50, 64, 100]
- Dropout: [0.1, 0.2, 0.3]

**Descoberta principal:** Dobrar o hidden_size (50â†’100) foi a mudanÃ§a mais impactante, reduzindo o MAPE de 6.74% para 3.83%.

---

## ğŸ§ª Como Carregar o Modelo

```python
# Carregar checkpoint
checkpoint = torch.load('models/model_lstm.pth')

# Recriar modelo com mesma arquitetura
model = StockLSTM(**checkpoint['model_config'])

# Carregar pesos
model.load_state_dict(checkpoint['model_state_dict'])

# Modo de inferÃªncia
model.eval()
```

---

## âœ… Checklist de ConclusÃ£o

### Treinamento Base
- [x] MSELoss configurado
- [x] Adam optimizer com lr=0.001
- [x] Loop de treinamento (100 Ã©pocas)
- [x] Forward e Backward pass funcionando
- [x] Train e Val loss monitorados
- [x] Best model salvo
- [x] GrÃ¡fico de histÃ³rico gerado
- [x] Model checkpoint salvo em .pth

### OtimizaÃ§Ã£o de HiperparÃ¢metros
- [x] Script de tuning criado (`src/hyperparameter_tuning.py`)
- [x] 12 experimentos executados
- [x] ParÃ¢metro mais impactante identificado (hidden_size)
- [x] Modelo re-treinado com hidden_size=100
- [x] MAPE reduzido de 6.74% para 3.83%
- [x] Meta de MAPE < 5% atingida

---

## ğŸ”— PrÃ³xima Etapa

**â†’ ETAPA 6: AvaliaÃ§Ã£o** (ConcluÃ­da)
- Calcular mÃ©tricas: MAE, RMSE, MAPE
- Plotar previsÃµes vs valores reais
- Reverter normalizaÃ§Ã£o para R$
