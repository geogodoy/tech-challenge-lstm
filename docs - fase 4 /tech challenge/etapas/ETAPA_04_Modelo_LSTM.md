# üìå ETAPA 4: Modelo LSTM

## üìã Resumo
| Item | Valor |
|------|-------|
| **Status** | ‚úÖ Conclu√≠da |
| **Data** | 2026-02-17 |
| **Tempo Estimado** | 45 min |
| **Tempo Real** | ~10 min |

---

## üéØ Objetivo
Definir a arquitetura da rede neural LSTM para previs√£o de pre√ßos de a√ß√µes.

---

## üéì Conex√£o com as Aulas

### Aula 03 - Arquiteturas de Redes Neurais Profundas
**Arquivo:** `docs - fase 4 /etapa 1 - redes neurais e deep learning/Aula 03 - Arquiteturas de Redes Neurais Profundas.txt`

#### Por que LSTM e n√£o RNN comum?

> *"As RNNs enfrentam desafios como o problema do desvanecimento e da explos√£o de gradientes durante o treinamento, especialmente em sequ√™ncias longas."* (Linha ~439)

> *"Para combater esses problemas, variantes de RNNs como Long Short-Term Memory (LSTM) e Gated Recurrent Units (GRU) foram desenvolvidas. Estas arquiteturas incluem mecanismos de port√µes que regulam o fluxo de informa√ß√µes."* (Linha ~443)

#### Estrutura da LSTM

> *"Eles permitem que a rede aprenda quais dados no estado devem ser lembrados ou esquecidos, melhorando a capacidade da rede de aprender depend√™ncias de longo prazo."* (Linha ~443)

#### Exemplo de c√≥digo na aula (Linhas 114-129):
```python
class RNN(nn.Module):
    def __init__(self):
        super(RNN, self).__init__()
        self.rnn = nn.LSTM(input_size=28, hidden_size=128,
                           num_layers=2, batch_first=True)
        self.output_layer = nn.Linear(128, 10)
```

---

## üìÅ Arquivo Implementado

### `src/model.py`

#### Classe Principal: `StockLSTM` (Linhas 14-127)

```python
class StockLSTM(nn.Module):
    def __init__(self, input_size=1, hidden_size=50, 
                 num_layers=2, dropout=0.2):
        super(StockLSTM, self).__init__()
        
        # 1Ô∏è‚É£ Camada LSTM
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # 2Ô∏è‚É£ Dropout para regulariza√ß√£o
        self.dropout = nn.Dropout(dropout)
        
        # 3Ô∏è‚É£ Camada Linear para sa√≠da
        self.linear = nn.Linear(hidden_size, 1)
```

---

## üß† Arquitetura Detalhada

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    StockLSTM - Arquitetura                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  INPUT: (batch_size, seq_length, input_size)                ‚îÇ
‚îÇ         Exemplo: (32, 60, 1)                                ‚îÇ
‚îÇ         32 amostras, 60 dias, 1 feature (Close)             ‚îÇ
‚îÇ                           ‚îÇ                                 ‚îÇ
‚îÇ                           ‚ñº                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                    nn.LSTM                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ input_size: 1 (apenas pre√ßo Close)               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ hidden_size: 50 (dimens√£o do estado oculto)      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ num_layers: 2 (LSTMs empilhadas)                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ batch_first: True                                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ dropout: 0.2 (entre camadas)                     ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                           ‚îÇ                                 ‚îÇ
‚îÇ                           ‚ñº                                 ‚îÇ
‚îÇ         lstm_out: (batch, seq_length, hidden_size)          ‚îÇ
‚îÇ         Pegamos apenas: lstm_out[:, -1, :]                  ‚îÇ
‚îÇ         = √∫ltimo passo temporal                             ‚îÇ
‚îÇ                           ‚îÇ                                 ‚îÇ
‚îÇ                           ‚ñº                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                  nn.Dropout(0.2)                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Desliga 20% dos neur√¥nios aleatoriamente           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Üí Regulariza√ß√£o para evitar overfitting            ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                           ‚îÇ                                 ‚îÇ
‚îÇ                           ‚ñº                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ               nn.Linear(50 ‚Üí 1)                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Transforma hidden_size em pre√ßo previsto           ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                           ‚îÇ                                 ‚îÇ
‚îÇ                           ‚ñº                                 ‚îÇ
‚îÇ  OUTPUT: (batch_size, 1) = Pre√ßo previsto                  ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üî¨ Hiperpar√¢metros e Conex√£o com a Teoria

### Tabela de Hiperpar√¢metros

| Hiperpar√¢metro | Valor | Justificativa | Refer√™ncia na Aula |
|----------------|-------|---------------|-------------------|
| `input_size` | 1 | Apenas pre√ßo Close | Feature √∫nica para simplificar |
| `hidden_size` | 50 | Capacidade de mem√≥ria | Similar ao exemplo da aula (128) |
| `num_layers` | 2 | Deep LSTM | "RNNs empilhadas" (linha ~446) |
| `dropout` | 0.2 | Regulariza√ß√£o | Linha ~331, ~445 |
| `batch_first` | True | Formato (batch, seq, features) | Padr√£o PyTorch |

### Dropout - Conex√£o com a Aula

> *"A regulariza√ß√£o tamb√©m √© uma parte crucial do treinamento... M√©todos como dropout s√£o frequentemente adaptados para RNNs, sendo aplicados n√£o apenas √†s entradas e sa√≠das da rede, mas tamb√©m entre os passos de tempo."* (Linha ~445)

**No c√≥digo:**
```python
# Linha 78: Dropout ENTRE camadas LSTM
dropout=dropout if num_layers > 1 else 0

# Linha 83: Dropout AP√ìS a LSTM (antes da camada linear)
self.dropout = nn.Dropout(dropout)
```

---

## üíª M√©todo `forward()` Detalhado

```python
def forward(self, x: torch.Tensor) -> torch.Tensor:
    # x shape: (batch_size, seq_length, input_size)
    # Exemplo: (32, 60, 1)
    
    # 1. Passar pela LSTM
    lstm_out, (h_n, c_n) = self.lstm(x)
    # lstm_out: (32, 60, 50) - sa√≠da de cada passo temporal
    # h_n: (2, 32, 50) - hidden state final (por camada)
    # c_n: (2, 32, 50) - cell state final (por camada)
    
    # 2. Pegar apenas o √öLTIMO passo da sequ√™ncia
    last_output = lstm_out[:, -1, :]
    # last_output: (32, 50) - a "mem√≥ria" ap√≥s processar 60 dias
    
    # 3. Aplicar dropout
    out = self.dropout(last_output)
    
    # 4. Camada linear para previs√£o
    prediction = self.linear(out)
    # prediction: (32, 1) - pre√ßo previsto para cada amostra
    
    return prediction
```

**Por que `lstm_out[:, -1, :]`?**
- Queremos a "mem√≥ria" da LSTM **ap√≥s** processar toda a sequ√™ncia
- O √∫ltimo hidden state cont√©m informa√ß√£o acumulada de todos os 60 dias
- √â como perguntar: "dado todo esse hist√≥rico, qual √© a previs√£o?"

---

## üìä Estat√≠sticas do Modelo

```
StockLSTM(
  (lstm): LSTM(1, 50, num_layers=2, batch_first=True, dropout=0.2)
  (dropout): Dropout(p=0.2, inplace=False)
  (linear): Linear(in_features=50, out_features=1, bias=True)
)

Total de par√¢metros trein√°veis: 31,051
```

### C√°lculo dos Par√¢metros

**Camada LSTM (2 camadas):**
- Camada 1: 4 √ó (1 √ó 50 + 50 √ó 50 + 50 + 50) = 4 √ó (50 + 2500 + 100) = 10,600
- Camada 2: 4 √ó (50 √ó 50 + 50 √ó 50 + 50 + 50) = 4 √ó (5000 + 100) = 20,400
- Total LSTM: ~31,000

**Camada Linear:**
- 50 √ó 1 + 1 (bias) = 51

---

## üß™ Teste do Modelo

```python
# Criar tensor de teste
x_test = torch.randn(32, 60, 1)  # 32 amostras, 60 dias, 1 feature
print(f"Input shape:  {x_test.shape}")   # torch.Size([32, 60, 1])

# Forward pass
output = model(x_test)
print(f"Output shape: {output.shape}")   # torch.Size([32, 1])
```

---

## ‚úÖ Checklist de Conclus√£o

- [x] Classe `StockLSTM` criada herdando `nn.Module`
- [x] Camada LSTM configurada (2 layers, hidden=50)
- [x] Dropout implementado (0.2)
- [x] Camada Linear para output
- [x] M√©todo `forward()` implementado
- [x] M√©todo `get_config()` para serializa√ß√£o
- [x] Factory function `create_model()`
- [x] Teste com dados sint√©ticos passou

---

## üîó Pr√≥xima Etapa

**‚Üí ETAPA 5: Treinamento**
- Configurar MSELoss e Adam
- Implementar loop de treinamento
- Monitorar train_loss e val_loss
