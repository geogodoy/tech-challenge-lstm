# ğŸ“Œ ETAPA 3: PrÃ©-processamento

## ğŸ“‹ Resumo
| Item | Valor |
|------|-------|
| **Status** | âœ… ConcluÃ­da |
| **Data** | 2026-02-17 |
| **Tempo Estimado** | 45 min |
| **Tempo Real** | ~10 min |

---

## ğŸ¯ Objetivo
Transformar os dados brutos em um formato adequado para a LSTM: normalizaÃ§Ã£o, criaÃ§Ã£o de sequÃªncias temporais e conversÃ£o para tensores PyTorch.

---

## ğŸ“ ConexÃ£o com as Aulas

### Aula 02 - Teoria das Redes Neurais Profundas
**Arquivo:** `docs - fase 4 /etapa 1 - redes neurais e deep learning/Aula 02 - Teoria das Redes Neurais Profundas.txt`

**Conceito: NormalizaÃ§Ã£o**
> A normalizaÃ§Ã£o Ã© essencial para evitar que valores grandes dominem o cÃ¡lculo do erro durante o treinamento.

**Por que normalizar entre 0 e 1?**
- Redes neurais funcionam melhor com valores pequenos e uniformes
- Evita problemas de *exploding gradients*
- Acelera a convergÃªncia do treinamento

### Aula 03 - Arquiteturas de Redes Neurais Profundas
**Arquivo:** `docs - fase 4 /etapa 1 - redes neurais e deep learning/Aula 03 - Arquiteturas de Redes Neurais Profundas.txt`

**Conceito: Processamento de SequÃªncias**
> *"As RNNs processam a entrada passo a passo, mantendo um estado interno que captura informaÃ§Ãµes de entradas anteriores."* (Linha ~421)

> *"A capacidade de transiÃ§Ã£o entre estados permite que a RNN capture dependÃªncias temporais ou sequenciais dos dados."* (Linha ~438)

**Por que janelas deslizantes?**
- RNNs/LSTMs processam **sequÃªncias** de dados
- Precisamos "fatiar" a sÃ©rie temporal em blocos
- Cada bloco = contexto histÃ³rico para prever o prÃ³ximo valor

---

## ğŸ“ Arquivo Implementado

### `src/preprocessing.py`

#### Estrutura do CÃ³digo

```python
# Linhas 1-5: CabeÃ§alho
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ ETAPA 3: PrÃ©-processamento
# ğŸ¯ Objetivo: Normalizar dados e criar janelas temporais
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

#### ConfiguraÃ§Ãµes (Linhas 22-29)
```python
SEQ_LENGTH = 60      # 60 dias = ~3 meses de histÃ³rico
TRAIN_SPLIT = 0.8    # 80% treino, 20% teste
MODELS_DIR = Path(__file__).parent.parent / "models"
```

---

## ğŸ”¬ FunÃ§Ãµes Implementadas

### 1ï¸âƒ£ `normalize_data()` (Linhas 32-52)

**PropÃ³sito:** Escalar os dados entre 0 e 1

```python
def normalize_data(data: np.ndarray) -> Tuple[np.ndarray, MinMaxScaler]:
    scaler = MinMaxScaler(feature_range=(0, 1))
    data_scaled = scaler.fit_transform(data)
    return data_scaled, scaler
```

**ConexÃ£o com a Aula:**
| Conceito | Aula | CÃ³digo |
|----------|------|--------|
| "NormalizaÃ§Ã£o evita valores grandes" | Aula 02 | `MinMaxScaler(feature_range=(0, 1))` |
| "Scaler deve ser salvo para inferÃªncia" | PrÃ¡tica ML | `return data_scaled, scaler` |

**Resultado:**
```
Original - Min: 3.24, Max: 27.38
Normalizado - Min: 0.0000, Max: 1.0000
```

---

### 2ï¸âƒ£ `create_sequences()` (Linhas 55-94)

**PropÃ³sito:** Criar janelas deslizantes para a LSTM

```python
def create_sequences(data: np.ndarray, seq_length: int = SEQ_LENGTH):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i+seq_length])      # 60 dias de entrada
        y.append(data[i+seq_length])         # 1 dia de saÃ­da (prÃ³ximo)
    return np.array(X), np.array(y)
```

**ConexÃ£o com a Aula:**
> *"A capacidade de lembrar informaÃ§Ãµes anteriores permite que as RNNs considerem o contexto amplo."* (Aula 03, linha ~454)

**VisualizaÃ§Ã£o:**
```
Dados: [P1, P2, P3, P4, P5, P6, ..., P60, P61, P62, ...]

SequÃªncia 1: [P1, P2, ..., P60]  â†’ Prever P61
SequÃªncia 2: [P2, P3, ..., P61]  â†’ Prever P62
SequÃªncia 3: [P3, P4, ..., P62]  â†’ Prever P63
...
```

**Por que 60 dias?**
- ~3 meses de histÃ³rico = contexto temporal suficiente
- Captura tendÃªncias de curto/mÃ©dio prazo
- RecomendaÃ§Ã£o do guia do Tech Challenge

---

### 3ï¸âƒ£ `train_test_split()` (Linhas 97-127)

**PropÃ³sito:** Dividir dados em treino e teste

```python
def train_test_split(X, y, train_ratio=0.8):
    split_idx = int(len(X) * train_ratio)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    return X_train, X_test, y_train, y_test
```

**ConexÃ£o com a Aula:**
> *"A regularizaÃ§Ã£o Ã© uma tÃ©cnica fundamental para evitar o sobreajuste, garantindo que o modelo generalize bem para novos dados."* (Aula 03, linha ~331)

**Por que separar?**
- **Treino (80%):** Dados para o modelo aprender
- **Teste (20%):** Dados que o modelo NUNCA viu â†’ avalia generalizaÃ§Ã£o

**âš ï¸ Importante:** Em sÃ©ries temporais, **NÃƒO** embaralhamos os dados! A ordem cronolÃ³gica Ã© preservada.

---

### 4ï¸âƒ£ `to_tensors()` (Linhas 130-159)

**PropÃ³sito:** Converter NumPy arrays para tensores PyTorch

```python
def to_tensors(X_train, X_test, y_train, y_test):
    X_train_t = torch.FloatTensor(X_train)
    X_test_t = torch.FloatTensor(X_test)
    y_train_t = torch.FloatTensor(y_train)
    y_test_t = torch.FloatTensor(y_test)
    return X_train_t, X_test_t, y_train_t, y_test_t
```

**ConexÃ£o com a Aula:**
> *"PyTorch trabalha com tensores, que sÃ£o estruturas otimizadas para operaÃ§Ãµes matriciais em GPU/CPU."* (Conceito fundamental de Deep Learning)

**Por que FloatTensor?**
- PrecisÃ£o de 32 bits (float32)
- Suficiente para treinamento
- CompatÃ­vel com operaÃ§Ãµes da GPU

---

### 5ï¸âƒ£ `preprocess_data()` (Linhas 162-233) - Pipeline Completa

**PropÃ³sito:** Orquestrar todo o prÃ©-processamento

```python
def preprocess_data(ticker, seq_length, train_ratio, save_scaler):
    # 1. Carregar dados
    df = load_stock_data(ticker)
    
    # 2. Selecionar coluna Close
    data = df['Close'].values.reshape(-1, 1)
    
    # 3. Normalizar
    data_scaled, scaler = normalize_data(data)
    
    # 4. Criar sequÃªncias
    X, y = create_sequences(data_scaled, seq_length)
    
    # 5. Dividir treino/teste
    X_train, X_test, y_train, y_test = train_test_split(X, y, train_ratio)
    
    # 6. Converter para tensores
    X_train_t, X_test_t, y_train_t, y_test_t = to_tensors(...)
    
    # 7. Salvar scaler e config
    joblib.dump(scaler, 'models/scaler.pkl')
    joblib.dump(config, 'models/config.pkl')
    
    return X_train_t, X_test_t, y_train_t, y_test_t, scaler
```

---

## ğŸ“Š Resultado do PrÃ©-processamento

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Dados Originais â†’ Dados Processados      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Registros originais:   1487                    â”‚
â”‚  ApÃ³s sequenciamento:   1427 amostras           â”‚
â”‚  (1487 - 60 = 1427)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  TREINO (80%)         â”‚  TESTE (20%)            â”‚
â”‚  X: (1141, 60, 1)     â”‚  X: (286, 60, 1)        â”‚
â”‚  y: (1141, 1)         â”‚  y: (286, 1)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Shape explicado:                               â”‚
â”‚  (amostras, seq_length, features)               â”‚
â”‚  (1141, 60, 1) = 1141 sequÃªncias de 60 dias     â”‚
â”‚                  com 1 feature (Close)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¾ Artefatos Salvos

| Arquivo | DescriÃ§Ã£o | Uso |
|---------|-----------|-----|
| `models/scaler.pkl` | MinMaxScaler treinado | Reverter normalizaÃ§Ã£o na inferÃªncia |
| `models/config.pkl` | ConfiguraÃ§Ãµes (seq_length, ticker) | Carregar modelo corretamente |

---

## âœ… Checklist de ConclusÃ£o

- [x] NormalizaÃ§Ã£o implementada (0-1)
- [x] Janelas deslizantes de 60 dias
- [x] Split 80/20 sem embaralhamento
- [x] ConversÃ£o para tensores PyTorch
- [x] Scaler salvo para inferÃªncia
- [x] Config salvo para reprodutibilidade

---

## ğŸ”— PrÃ³xima Etapa

**â†’ ETAPA 4: Modelo LSTM**
- Criar classe `StockLSTM` com PyTorch
- Definir camadas: LSTM â†’ Dropout â†’ Linear
- Configurar hiperparÃ¢metros
