# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ ETAPA 4: Modelo LSTM
# ğŸ¯ Objetivo: Definir a arquitetura da rede neural
# ğŸ“ ReferÃªncia: Guia linhas 186-239
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import torch
import torch.nn as nn

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ§  ARQUITETURA LSTM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class StockLSTM(nn.Module):
    """
    Modelo LSTM para previsÃ£o de preÃ§os de aÃ§Ãµes.
    
    Arquitetura:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Input: (batch_size, seq_length, input_size)                â”‚
    â”‚         Ex: (32, 60, 1) = 32 amostras, 60 dias, 1 feature   â”‚
    â”‚                                                              â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
    â”‚  â”‚  LSTM   â”‚  â† Captura padrÃµes temporais (memÃ³ria)         â”‚
    â”‚  â”‚ layers  â”‚    hidden_size=50, num_layers=2                â”‚
    â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                                â”‚
    â”‚       â”‚                                                      â”‚
    â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                                                â”‚
    â”‚  â”‚ Dropout â”‚  â† RegularizaÃ§Ã£o (evita overfitting)           â”‚
    â”‚  â”‚  (0.2)  â”‚    Desliga 20% dos neurÃ´nios aleatoriamente    â”‚
    â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                                â”‚
    â”‚       â”‚                                                      â”‚
    â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                                                â”‚
    â”‚  â”‚ Linear  â”‚  â† Transforma hidden_size â†’ 1 (preÃ§o)          â”‚
    â”‚  â”‚ (50â†’1)  â”‚                                                â”‚
    â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                                â”‚
    â”‚       â”‚                                                      â”‚
    â”‚  Output: (batch_size, 1) = PreÃ§o previsto                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Por que LSTM?
    - RNNs comuns "esquecem" muito rÃ¡pido (vanishing gradient)
    - LSTM possui "portÃµes" (gates) que decidem o que manter na memÃ³ria
    - Ideal para sÃ©ries temporais como preÃ§os de aÃ§Ãµes
    """
    
    def __init__(
        self, 
        input_size: int = 1, 
        hidden_size: int = 50, 
        num_layers: int = 2, 
        dropout: float = 0.2
    ):
        """
        Inicializa o modelo LSTM.
        
        Args:
            input_size: NÃºmero de features de entrada (1 = apenas Close)
            hidden_size: DimensÃ£o do estado oculto da LSTM
            num_layers: NÃºmero de camadas LSTM empilhadas
            dropout: Taxa de dropout para regularizaÃ§Ã£o
        """
        super(StockLSTM, self).__init__()
        
        # Salvar configuraÃ§Ãµes para uso posterior
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout_rate = dropout
        
        # 1ï¸âƒ£ Camada LSTM: O coraÃ§Ã£o que guarda o contexto temporal
        # batch_first=True: entrada no formato (batch, seq, features)
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0  # Dropout entre camadas LSTM
        )
        
        # 2ï¸âƒ£ Dropout: Evita que o modelo "decore" os preÃ§os passados (overfitting)
        # Durante o treino, desliga neurÃ´nios aleatoriamente
        self.dropout = nn.Dropout(dropout)
        
        # 3ï¸âƒ£ Camada Linear: Transforma a memÃ³ria da LSTM no preÃ§o final previsto
        # Entrada: hidden_size (50), SaÃ­da: 1 (preÃ§o)
        self.linear = nn.Linear(hidden_size, 1)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass: processa a entrada atravÃ©s da rede.
        
        Args:
            x: Tensor de entrada com shape (batch_size, seq_length, input_size)
               Exemplo: (32, 60, 1) = 32 amostras de 60 dias cada
               
        Returns:
            Tensor com previsÃµes de shape (batch_size, 1)
        """
        # Passar pela LSTM
        # lstm_out shape: (batch_size, seq_length, hidden_size)
        # h_n shape: (num_layers, batch_size, hidden_size) - Ãºltimo hidden state
        # c_n shape: (num_layers, batch_size, hidden_size) - Ãºltimo cell state
        lstm_out, (h_n, c_n) = self.lstm(x)
        
        # Pegar apenas o Ãºltimo passo da sequÃªncia
        # Queremos a "memÃ³ria" apÃ³s processar todos os 60 dias
        # last_output shape: (batch_size, hidden_size)
        last_output = lstm_out[:, -1, :]
        
        # Aplicar dropout (regularizaÃ§Ã£o)
        out = self.dropout(last_output)
        
        # Camada linear para obter o preÃ§o previsto
        # prediction shape: (batch_size, 1)
        prediction = self.linear(out)
        
        return prediction
    
    def get_config(self) -> dict:
        """Retorna configuraÃ§Ã£o do modelo para salvamento."""
        return {
            'input_size': self.input_size,
            'hidden_size': self.hidden_size,
            'num_layers': self.num_layers,
            'dropout': self.dropout_rate
        }


def create_model(
    input_size: int = 1,
    hidden_size: int = 100,  # Otimizado: era 50, agora 100 para MAPE < 5%
    num_layers: int = 2,
    dropout: float = 0.2,
    device: str = None
) -> StockLSTM:
    """
    Factory function para criar e configurar o modelo.
    
    Args:
        input_size: NÃºmero de features
        hidden_size: DimensÃ£o do estado oculto
        num_layers: NÃºmero de camadas LSTM
        dropout: Taxa de dropout
        device: Dispositivo ('cuda', 'cpu', ou None para auto)
        
    Returns:
        Modelo configurado e movido para o dispositivo apropriado
    """
    # Determinar dispositivo automaticamente se nÃ£o especificado
    if device is None:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # Criar modelo
    model = StockLSTM(
        input_size=input_size,
        hidden_size=hidden_size,
        num_layers=num_layers,
        dropout=dropout
    )
    
    # Mover para dispositivo
    model = model.to(device)
    
    return model


def count_parameters(model: nn.Module) -> int:
    """Conta o nÃºmero de parÃ¢metros treinÃ¡veis do modelo."""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸš€ EXECUÃ‡ÃƒO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    print("="*60)
    print("ğŸ“Œ ETAPA 4: ConstruÃ§Ã£o do Modelo LSTM")
    print("="*60)
    
    # Configurar dispositivo
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"\nğŸ–¥ï¸ Dispositivo: {device}")
    if device == 'cuda':
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
    
    # Criar modelo
    model = create_model(device=device)
    
    # Exibir arquitetura
    print(f"\nğŸ§  Arquitetura do modelo:")
    print(model)
    
    # Contar parÃ¢metros
    n_params = count_parameters(model)
    print(f"\nğŸ“Š Total de parÃ¢metros treinÃ¡veis: {n_params:,}")
    
    # Teste com dados simulados
    print(f"\nğŸ§ª Teste com dados simulados:")
    batch_size = 32
    seq_length = 60
    input_size = 1
    
    # Criar tensor de entrada aleatÃ³rio
    x_test = torch.randn(batch_size, seq_length, input_size).to(device)
    print(f"   Input shape:  {x_test.shape}")
    
    # Forward pass
    model.eval()
    with torch.no_grad():
        output = model(x_test)
    print(f"   Output shape: {output.shape}")
    
    # âœ… Checkpoint
    print("\n" + "="*60)
    print("ğŸ‰ CHECKPOINT: O cÃ©rebro nasceu!")
    print("="*60)
    print(f"\nğŸ“‹ ConfiguraÃ§Ã£o do modelo:")
    for key, value in model.get_config().items():
        print(f"   {key}: {value}")
