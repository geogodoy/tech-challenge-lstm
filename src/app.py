# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ ETAPA 8: API FastAPI
# ğŸ¯ Objetivo: Servir o modelo LSTM via endpoint REST
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import os
import time
from pathlib import Path
from typing import List, Optional
from contextlib import asynccontextmanager

import torch
import joblib
import numpy as np
from fastapi import FastAPI, HTTPException, status
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, field_validator

try:
    from model import StockLSTM
except ImportError:
    from src.model import StockLSTM

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“ CONFIGURAÃ‡ÃƒO DE PATHS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BASE_DIR = Path(__file__).parent.parent
MODELS_DIR = BASE_DIR / "models"

MODEL_PATH = MODELS_DIR / "model_lstm.pth"
SCALER_PATH = MODELS_DIR / "scaler.pkl"
CONFIG_PATH = MODELS_DIR / "config.pkl"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ—„ï¸ ESTADO GLOBAL DA APLICAÃ‡ÃƒO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ModelState:
    """Armazena o estado do modelo carregado."""
    model: Optional[StockLSTM] = None
    scaler = None
    config: Optional[dict] = None
    device: str = "cpu"
    is_loaded: bool = False

state = ModelState()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸš€ LIFESPAN (STARTUP/SHUTDOWN)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Gerencia o ciclo de vida da aplicaÃ§Ã£o."""
    # STARTUP: Carregar modelo
    print("="*60)
    print("ğŸš€ Iniciando Stock Price Predictor API...")
    print("="*60)
    
    try:
        # Verificar se arquivos existem
        for path, name in [(MODEL_PATH, "Modelo"), (SCALER_PATH, "Scaler"), (CONFIG_PATH, "Config")]:
            if not path.exists():
                raise FileNotFoundError(f"{name} nÃ£o encontrado: {path}")
        
        # Configurar dispositivo
        state.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"\nğŸ–¥ï¸  Dispositivo: {state.device}")
        
        # Carregar configuraÃ§Ãµes
        state.config = joblib.load(CONFIG_PATH)
        print(f"ğŸ“‹ Config carregado: seq_length={state.config.get('seq_length', 60)}")
        
        # Carregar scaler
        state.scaler = joblib.load(SCALER_PATH)
        print(f"ğŸ“Š Scaler carregado: MinMaxScaler")
        
        # Carregar modelo
        checkpoint = torch.load(MODEL_PATH, map_location=state.device, weights_only=False)
        model_config = checkpoint.get('model_config', {})
        
        # Criar instÃ¢ncia do modelo com a configuraÃ§Ã£o salva
        state.model = StockLSTM(
            input_size=model_config.get('input_size', 1),
            hidden_size=model_config.get('hidden_size', 100),
            num_layers=model_config.get('num_layers', 2),
            dropout=model_config.get('dropout', 0.2)
        )
        
        # Carregar pesos
        state.model.load_state_dict(checkpoint['model_state_dict'])
        state.model.to(state.device)
        state.model.eval()
        
        state.is_loaded = True
        
        print(f"ğŸ§  Modelo carregado: hidden_size={model_config.get('hidden_size', 100)}")
        print("\n" + "="*60)
        print("âœ… API pronta para receber requisiÃ§Ãµes!")
        print("="*60 + "\n")
        
    except Exception as e:
        print(f"\nâŒ Erro ao carregar modelo: {e}")
        state.is_loaded = False
    
    yield
    
    # SHUTDOWN
    print("\nğŸ›‘ Encerrando API...")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“¦ SCHEMAS (PYDANTIC)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class PredictionRequest(BaseModel):
    """Schema de entrada para previsÃ£o."""
    prices: List[float] = Field(
        ...,
        description="Lista com os Ãºltimos N preÃ§os de fechamento (mÃ­nimo: seq_length dias)",
        examples=[[25.50, 26.10, 25.80, 26.30, 26.50]]
    )
    
    @field_validator('prices')
    @classmethod
    def validate_prices(cls, v):
        if not v:
            raise ValueError("Lista de preÃ§os nÃ£o pode estar vazia")
        if any(p <= 0 for p in v):
            raise ValueError("Todos os preÃ§os devem ser positivos")
        return v


class PredictionResponse(BaseModel):
    """Schema de saÃ­da para previsÃ£o."""
    predicted_price: float = Field(..., description="PreÃ§o previsto para o prÃ³ximo dia")
    currency: str = Field(default="BRL", description="Moeda")
    ticker: str = Field(..., description="Ticker da aÃ§Ã£o")
    input_days: int = Field(..., description="Quantidade de dias usados na previsÃ£o")
    processing_time_ms: float = Field(..., description="Tempo de processamento em ms")
    model_info: dict = Field(..., description="InformaÃ§Ãµes do modelo")


class HealthResponse(BaseModel):
    """Schema de resposta do health check."""
    status: str
    model_loaded: bool
    device: str
    ticker: Optional[str] = None
    seq_length: Optional[int] = None


class ErrorResponse(BaseModel):
    """Schema de resposta de erro."""
    detail: str
    error_type: str

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸŒ APLICAÃ‡ÃƒO FASTAPI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

app = FastAPI(
    title="Stock Price Predictor API",
    description="""
    ## ğŸ“ˆ API de PrevisÃ£o de PreÃ§os de AÃ§Ãµes com LSTM
    
    Esta API utiliza um modelo de Deep Learning (LSTM) para prever o preÃ§o 
    de fechamento do prÃ³ximo dia com base nos Ãºltimos 60 dias de histÃ³rico.
    
    ### Endpoints:
    - **POST /predict**: Envia preÃ§os histÃ³ricos e recebe a previsÃ£o
    - **GET /health**: Verifica o status da API e do modelo
    
    ### Tech Challenge - Fase 4
    PÃ³s-graduaÃ§Ã£o em Machine Learning Engineering
    """,
    version="1.0.0",
    lifespan=lifespan
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”Œ ENDPOINTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get(
    "/health",
    response_model=HealthResponse,
    tags=["Status"],
    summary="Health Check",
    description="Verifica se a API estÃ¡ funcionando e o modelo estÃ¡ carregado."
)
async def health_check():
    """Retorna o status da API e do modelo."""
    return HealthResponse(
        status="healthy" if state.is_loaded else "unhealthy",
        model_loaded=state.is_loaded,
        device=state.device,
        ticker=state.config.get('ticker') if state.config else None,
        seq_length=state.config.get('seq_length') if state.config else None
    )


@app.post(
    "/predict",
    response_model=PredictionResponse,
    tags=["PrevisÃ£o"],
    summary="Prever PrÃ³ximo PreÃ§o",
    description="Recebe uma lista de preÃ§os histÃ³ricos e retorna a previsÃ£o do prÃ³ximo dia.",
    responses={
        400: {"model": ErrorResponse, "description": "Dados invÃ¡lidos"},
        503: {"model": ErrorResponse, "description": "Modelo nÃ£o carregado"}
    }
)
async def predict(request: PredictionRequest):
    """
    Realiza a previsÃ£o do preÃ§o do prÃ³ximo dia.
    
    - **prices**: Lista com pelo menos `seq_length` (60) preÃ§os de fechamento
    
    Retorna o preÃ§o previsto para o prÃ³ximo dia Ãºtil.
    """
    start_time = time.time()
    
    # Verificar se modelo estÃ¡ carregado
    if not state.is_loaded:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Modelo nÃ£o estÃ¡ carregado. Verifique os logs do servidor."
        )
    
    seq_length = state.config.get('seq_length', 60)
    
    # Validar quantidade de preÃ§os
    if len(request.prices) < seq_length:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"NecessÃ¡rio pelo menos {seq_length} preÃ§os histÃ³ricos. Recebido: {len(request.prices)}"
        )
    
    try:
        # Pegar os Ãºltimos seq_length preÃ§os
        prices = np.array(request.prices[-seq_length:]).reshape(-1, 1)
        
        # Normalizar usando o scaler treinado
        prices_scaled = state.scaler.transform(prices)
        
        # Converter para tensor PyTorch
        X = torch.FloatTensor(prices_scaled).unsqueeze(0).to(state.device)
        
        # Fazer previsÃ£o
        with torch.no_grad():
            prediction_scaled = state.model(X).cpu().numpy()
        
        # Reverter normalizaÃ§Ã£o para obter preÃ§o em R$
        predicted_price = state.scaler.inverse_transform(prediction_scaled)[0][0]
        
        # Calcular tempo de processamento
        processing_time = (time.time() - start_time) * 1000
        
        return PredictionResponse(
            predicted_price=round(float(predicted_price), 2),
            currency="BRL",
            ticker=state.config.get('ticker', 'PETR4.SA'),
            input_days=seq_length,
            processing_time_ms=round(processing_time, 2),
            model_info={
                "type": "LSTM",
                "hidden_size": state.model.hidden_size,
                "num_layers": state.model.num_layers,
                "device": state.device
            }
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erro ao processar previsÃ£o: {str(e)}"
        )


@app.get(
    "/",
    tags=["Status"],
    summary="Root",
    description="Redireciona para a documentaÃ§Ã£o da API."
)
async def root():
    """Endpoint raiz com informaÃ§Ãµes bÃ¡sicas."""
    return {
        "message": "Stock Price Predictor API",
        "version": "1.0.0",
        "docs": "/docs",
        "health": "/health"
    }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸš€ EXECUÃ‡ÃƒO LOCAL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import uvicorn
    
    print("\n" + "="*60)
    print("ğŸ“Œ ETAPA 8: API FastAPI")
    print("="*60)
    print("\nğŸŒ Iniciando servidor em http://localhost:8000")
    print("ğŸ“š DocumentaÃ§Ã£o disponÃ­vel em http://localhost:8000/docs")
    print("\nğŸ’¡ Pressione Ctrl+C para encerrar\n")
    
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=8000,
        reload=True
    )
