# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Œ ETAPA 6: AvaliaÃ§Ã£o do Modelo
# ğŸ¯ Objetivo: Calcular mÃ©tricas e avaliar performance
# ğŸ“ ReferÃªncia: GUIA_TREINAMENTO_E_AVALIACAO.md - Parte 2
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import torch
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error
from pathlib import Path

from model import StockLSTM
from preprocessing import preprocess_data

# DiretÃ³rios
MODELS_DIR = Path(__file__).parent.parent / "models"


def load_trained_model(model_path: Path = None) -> tuple:
    """
    Carrega o modelo treinado e suas configuraÃ§Ãµes.
    
    Returns:
        Tuple com (model, checkpoint)
    """
    if model_path is None:
        model_path = MODELS_DIR / "model_lstm.pth"
    
    print(f"ğŸ“¥ Carregando modelo de: {model_path}")
    
    checkpoint = torch.load(model_path, weights_only=False)
    
    model = StockLSTM(**checkpoint['model_config'])
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"âœ… Modelo carregado!")
    print(f"   Train Loss final: {checkpoint['final_train_loss']:.6f}")
    print(f"   Val Loss final:   {checkpoint['final_val_loss']:.6f}")
    
    return model, checkpoint


def make_predictions(model, X_test, scaler):
    """
    Faz previsÃµes e reverte a normalizaÃ§Ã£o.
    
    Returns:
        Tuple com (predictions_reais, actual_reais) em R$
    """
    model.eval()
    with torch.no_grad():
        predictions = model(X_test)
    
    predictions_np = predictions.numpy()
    
    return predictions_np


def calculate_metrics(actual_reais, predictions_reais):
    """
    Calcula mÃ©tricas de avaliaÃ§Ã£o.
    
    Returns:
        Dict com MSE, RMSE, MAE, MAPE
    """
    mse = mean_squared_error(actual_reais, predictions_reais)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(actual_reais, predictions_reais)
    mape = np.mean(np.abs((actual_reais - predictions_reais) / actual_reais)) * 100
    
    return {
        'mse': mse,
        'rmse': rmse,
        'mae': mae,
        'mape': mape
    }


def plot_predictions(actual_reais, predictions_reais, save_path=None):
    """
    Plota grÃ¡fico de previsÃµes vs valores reais.
    """
    plt.figure(figsize=(14, 6))
    
    # GrÃ¡fico 1: ComparaÃ§Ã£o temporal
    plt.subplot(1, 2, 1)
    n_samples = min(100, len(actual_reais))
    plt.plot(actual_reais[-n_samples:], label='Real', color='blue', linewidth=2)
    plt.plot(predictions_reais[-n_samples:], label='Previsto', color='red', 
             linewidth=2, linestyle='--')
    plt.title(f'PrevisÃ£o vs Valor Real (Ãšltimas {n_samples} amostras)')
    plt.xlabel('Amostra')
    plt.ylabel('PreÃ§o (R$)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # GrÃ¡fico 2: Scatter plot
    plt.subplot(1, 2, 2)
    plt.scatter(actual_reais, predictions_reais, alpha=0.5, s=20)
    min_val = min(actual_reais.min(), predictions_reais.min())
    max_val = max(actual_reais.max(), predictions_reais.max())
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Ideal')
    plt.xlabel('Valor Real (R$)')
    plt.ylabel('Valor Previsto (R$)')
    plt.title('CorrelaÃ§Ã£o: Previsto vs Real')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"ğŸ“Š GrÃ¡fico salvo em: {save_path}")
    
    plt.close()


def diagnose_model(mape):
    """
    DiagnÃ³stico baseado no MAPE.
    """
    print(f"\nğŸ” DIAGNÃ“STICO:")
    if mape < 5:
        print("   âœ… Excelente! Modelo muito preciso.")
        return "excelente"
    elif mape < 10:
        print("   âœ… Bom! Modelo com boa precisÃ£o.")
        return "bom"
    elif mape < 20:
        print("   âš ï¸ AceitÃ¡vel. Considere ajustes para melhorar.")
        return "aceitavel"
    else:
        print("   âŒ Precisa melhorar. Revise hiperparÃ¢metros e dados.")
        return "ruim"


def evaluate_model():
    """
    Pipeline completa de avaliaÃ§Ã£o.
    """
    print("=" * 60)
    print("ğŸ“Œ ETAPA 6: AvaliaÃ§Ã£o do Modelo LSTM")
    print("=" * 60)
    
    # 1. Carregar modelo
    print("\nğŸ“¥ Carregando modelo treinado...")
    model, checkpoint = load_trained_model()
    
    # 2. Carregar dados
    print("\nğŸ“¥ Carregando dados de teste...")
    X_train, X_test, y_train, y_test, scaler = preprocess_data(save_scaler=False)
    print(f"   Amostras de teste: {len(X_test)}")
    
    # 3. Fazer previsÃµes
    print("\nğŸ”® Fazendo previsÃµes...")
    predictions_np = make_predictions(model, X_test, scaler)
    
    # 4. Reverter normalizaÃ§Ã£o para R$
    predictions_reais = scaler.inverse_transform(predictions_np)
    actual_reais = scaler.inverse_transform(y_test.numpy())
    
    print(f"   PrevisÃµes feitas: {len(predictions_reais)} amostras")
    
    # 5. Exibir exemplos
    print(f"\nğŸ“‹ Exemplos de previsÃµes:")
    print(f"   {'Previsto':>12} | {'Real':>12} | {'Erro':>12}")
    print("   " + "-" * 42)
    for i in range(5):
        prev = predictions_reais[i][0]
        real = actual_reais[i][0]
        erro = abs(prev - real)
        print(f"   R$ {prev:>9.2f} | R$ {real:>9.2f} | R$ {erro:>9.2f}")
    
    # 6. Calcular mÃ©tricas
    print("\n" + "=" * 50)
    print("ğŸ“Š MÃ‰TRICAS DE AVALIAÃ‡ÃƒO")
    print("=" * 50)
    
    metrics = calculate_metrics(actual_reais, predictions_reais)
    
    print(f"MSE  (Mean Squared Error):     {metrics['mse']:.4f}")
    print(f"RMSE (Root Mean Squared Error): R$ {metrics['rmse']:.2f}")
    print(f"MAE  (Mean Absolute Error):     R$ {metrics['mae']:.2f}")
    print(f"MAPE (Mean Absolute % Error):   {metrics['mape']:.2f}%")
    print("=" * 50)
    
    # 7. DiagnÃ³stico
    diagnosis = diagnose_model(metrics['mape'])
    
    # 8. Gerar grÃ¡ficos
    print("\nğŸ“Š Gerando grÃ¡ficos...")
    plot_path = MODELS_DIR / "predictions_vs_actual.png"
    plot_predictions(actual_reais, predictions_reais, save_path=plot_path)
    
    # 9. Resumo final
    print("\n" + "=" * 60)
    print("âœ… AVALIAÃ‡ÃƒO CONCLUÃDA!")
    print("=" * 60)
    print(f"\nğŸ“‹ Resumo:")
    print(f"   RMSE: R$ {metrics['rmse']:.2f} (erro mÃ©dio em reais)")
    print(f"   MAPE: {metrics['mape']:.2f}% (erro percentual mÃ©dio)")
    print(f"   Status: {diagnosis.upper()}")
    
    return metrics, diagnosis


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸš€ EXECUÃ‡ÃƒO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    metrics, diagnosis = evaluate_model()
